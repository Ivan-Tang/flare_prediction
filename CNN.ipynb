{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import datetime as dt\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDOBenchmarkDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Generates data for PyTorch.\n",
    "    Inspired by https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly.html\n",
    "    \"\"\"\n",
    "    def __init__(self, base_path, transform=None, augment=False, label_func=None, data_format=\"channels_last\"):\n",
    "        \"\"\"\n",
    "        Initialization.\n",
    "        :param base_path: Path to the dataset directory.\n",
    "        :param transform: Optional transform to be applied on a sample.\n",
    "        :param augment: Whether to augment the data by flipping horizontally.\n",
    "        :param label_func: Optional function to transform labels.\n",
    "        :param data_format: Data format, either 'channels_last' or 'channels_first'.\n",
    "        \"\"\"\n",
    "        self.base_path = base_path\n",
    "        self.transform = transform\n",
    "        self.label_func = label_func\n",
    "        self.data_format = data_format\n",
    "        self.channels = ['magnetogram']  # Assuming only magnetogram data\n",
    "        self.time_steps = [0, 7*60, 10*60+30, 11*60+50]  # Time steps in minutes\n",
    "        self.data = self.loadCSV(augment)  # Load metadata\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def loadCSV(self, augment=True):\n",
    "        \"\"\"\n",
    "        Load metadata from CSV file and optionally augment the data.\n",
    "        :param augment: Whether to augment the data by flipping horizontally.\n",
    "        :return: DataFrame containing metadata.\n",
    "        \"\"\"\n",
    "        data = pd.read_csv(os.path.join(self.base_path, 'meta_data_processed.csv'), sep=\",\", parse_dates=[\"start\", \"end\"], index_col=\"id\")\n",
    "        # Augment by doubling the data and flagging them to be flipped horizontally\n",
    "        data['flip'] = False\n",
    "        if augment:\n",
    "            new_data = data.copy()\n",
    "            new_data.index += '_copy'\n",
    "            new_data['flip'] = True\n",
    "            data = pd.concat([data, new_data])\n",
    "        return data\n",
    "    \n",
    "\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Updates indexes after each epoch.\n",
    "        \"\"\"\n",
    "        self.indexes = np.arange(len(self.data))\n",
    "        np.random.shuffle(self.indexes)\n",
    "\n",
    "    def loadImg(self, sample_id):\n",
    "        \"\"\"\n",
    "        Load the images of each timestep as channels.\n",
    "        :param sample_id: ID of the sample.\n",
    "        :return: Numpy array containing the images.\n",
    "        \"\"\"\n",
    "        ar_nr, p = sample_id.replace('_copy', '').split(\"_\", 1)\n",
    "        path = os.path.join(self.base_path, ar_nr, p)\n",
    "\n",
    "        slices = np.zeros((4, 256, 256))  # Initialize slices for 4 time steps\n",
    "\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"路径不存在: {path}\")\n",
    "            return slices\n",
    "\n",
    "        sample_date = dt.datetime.strptime(p[:p.rfind('_')], \"%Y_%m_%d_%H_%M_%S\")\n",
    "        time_steps = [sample_date + dt.timedelta(minutes=offset) for offset in self.time_steps]\n",
    "        for img in [name for name in os.listdir(path) if name.endswith('.jpg')]:\n",
    "            try:\n",
    "                img_datetime_raw, img_wavelength = os.path.splitext(img)[0].split(\"__\")\n",
    "                img_datetime = dt.datetime.strptime(img_datetime_raw, \"%Y-%m-%dT%H%M%S\")\n",
    "\n",
    "                # Calculate wavelength and datetime index\n",
    "                datetime_index = [di[0] for di in enumerate(time_steps) if abs(di[1] - img_datetime) < dt.timedelta(minutes=15)]\n",
    "                if img_wavelength in self.channels and len(datetime_index) > 0:\n",
    "                    img_path = os.path.join(path, img)\n",
    "                    val = np.array(Image.open(img_path).convert('L'))  # Load grayscale image\n",
    "                    if val.shape != (256, 256):\n",
    "                        print(f\"图像形状不正确: {img_path}, 形状: {val.shape}\")\n",
    "                        continue\n",
    "\n",
    "                    if self.data_format == 'channels_last':\n",
    "                        slices[datetime_index[0], :, :] = val\n",
    "                    else:\n",
    "                        slices[:, :, datetime_index[0]] = val\n",
    "            except Exception as e:\n",
    "                print(f\"加载图像时出错: {img_path}, 错误信息: {e}\")\n",
    "\n",
    "        return slices\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Denotes the number of samples.\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Generates one sample of data.\n",
    "        :param index: Index of the sample.\n",
    "        :return: Tuple (X, y) where X is the input data and y is the label.\n",
    "        \"\"\"\n",
    "        # Load data and get label\n",
    "        sample_id = self.data.index[index]\n",
    "        X = self.loadImg(sample_id)\n",
    "        y = self.data.loc[sample_id, 'standard_peak_flux']\n",
    "\n",
    "        # Apply label function if provided\n",
    "        if self.label_func is not None:\n",
    "            y = self.label_func(y)\n",
    "\n",
    "        # Data augmentation\n",
    "        if self.data.loc[sample_id, 'flip']:\n",
    "            X = np.flip(X, axis=2).copy()  # Horizontal flip and copy to avoid modifying the original array\n",
    "\n",
    "        # Convert to tensor\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "        # Apply transform if any\n",
    "        if self.transform:\n",
    "            X = self.transform(X)\n",
    "\n",
    "        return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "goes_classes = ['quiet', 'A', 'B', 'C', 'M', 'X']\n",
    "\n",
    "def flux_to_class(f: float, only_main=False):\n",
    "    if f == 0:\n",
    "        return 'quiet'\n",
    "    if f < 0:\n",
    "        # 打印出引起错误的值\n",
    "        print(f\"Error: Non-positive flux value detected in flux_to_class: {f:15f}\")\n",
    "        raise ValueError(\"Flux value must be positive for logarithm calculation.\")\n",
    "    decade = int(min(math.floor(math.log10(f)), -4))\n",
    "    sub = round(10 ** -decade * f)\n",
    "    if decade < -4:  # avoiding class 10\n",
    "        decade += sub // 10\n",
    "        sub = max(sub % 10, 1)\n",
    "    main_class = goes_classes[decade + 9] if decade >= -8 else 'quiet'\n",
    "    sub_class = str(sub) if main_class != 'quiet' and only_main != True else ''\n",
    "    return main_class + sub_class\n",
    "\n",
    "def class_to_flux(c: str):\n",
    "    if c == 'quiet':\n",
    "        return 1e-9\n",
    "    decade = goes_classes.index(c[0]) - 9\n",
    "    sub = float(c[1:]) if len(c) > 1 else 1\n",
    "    return round(10 ** decade * sub, 10)\n",
    "\n",
    "def true_skill_statistic(y_true, y_pred, threshold='M'):\n",
    "    separator = class_to_flux(threshold)\n",
    "    y_true = [1 if yt >= separator else 0 for yt in y_true]\n",
    "    y_pred = [1 if yp >= separator else 0 for yp in y_pred]\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return tp / (tp + fn) - fp / (fp + tn)\n",
    "\n",
    "def heidke_skill_score(y_true, y_pred, threshold='M'):\n",
    "    separator = class_to_flux(threshold)\n",
    "    y_true = [1 if yt >= separator else 0 for yt in y_true]\n",
    "    y_pred = [1 if yp >= separator else 0 for yp in y_pred]\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return (tp + fn) / len(y_pred) * (tp + fp) / len(y_pred) + (tn + fn) / len(y_pred) * (tn + fp) / len(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "通量最大值为 0.0010941176470588\n",
      "通量最小值为 1e-09\n",
      "取对数后通量最大值为 -6.8178070423192185\n",
      "取对数后通量最小值为 -20.72326583694641\n",
      "标准化后通量最大值为 2.4720720876181956\n",
      "标准化后通量最小值为 -1.100167580966047\n",
      "25%分位数: -1.100167580966047\n",
      "75%分位数: 0.8524645244562409\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n#可视化样本分布\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# 绘制直方图\\nplt.figure(figsize=(10, 6))\\nsns.histplot(train_df['peak_flux'], bins=100, alpha=0.5, label='training set')\\nsns.histplot(test_df['peak_flux'], bins=100, alpha=0.5, label='validation set')\\nplt.title('distrubution of peak flux')\\nplt.xlabel('peak flux')\\nplt.ylabel('frequency')\\nplt.xlim(1.1*min(train_df['peak_flux'].min(), test_df['peak_flux'].min()), 1.1*max(train_df['peak_flux'].max(), test_df['peak_flux'].max()))\\nplt.legend()\\n\\n\\n# 绘制直方图\\nplt.figure(figsize=(10, 6))\\nsns.histplot(train_df['standard_peak_flux'], bins=100, alpha=0.5, label='training set')\\nsns.histplot(test_df['standard_peak_flux'], bins=100, alpha=0.5, label='validation set')\\nplt.title('distrubution of peak flux(standardized)')\\nplt.xlabel('standardized peak flux')\\nplt.ylabel('frequency')\\nplt.xlim(1.1*min(train_df['standard_peak_flux'].min(), test_df['standard_peak_flux'].min()), 1.1*max(train_df['standard_peak_flux'].max(), test_df['standard_peak_flux'].max()))\\nplt.legend()\\nplt.show()\\n\""
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义 base path\n",
    "base_path = '/home/AIIAcourse/course/231503031/flare_prediction/data/archive/SDOBenchmark_full/'\n",
    "#检查样本数据分布\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(base_path, \"training/meta_data.csv\")).copy()\n",
    "test_df = pd.read_csv(os.path.join(base_path, \"test/meta_data.csv\")).copy()\n",
    "\n",
    "\n",
    "max_peak_flux = max(train_df['peak_flux'].max(), test_df['peak_flux'].max())\n",
    "min_peak_flux = min(train_df['peak_flux'].min(), test_df['peak_flux'].min())\n",
    "#打印样本分布\n",
    "print(f'通量最大值为 {max_peak_flux}')\n",
    "print(f'通量最小值为 {min_peak_flux}')\n",
    "\n",
    "\n",
    "#标准化样本分布\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "train_df['log_peak_flux'] = np.log(train_df['peak_flux'])\n",
    "test_df['log_peak_flux'] = np.log(test_df['peak_flux'])\n",
    "\n",
    "print(f'取对数后通量最大值为 {max(train_df[\"log_peak_flux\"].max(), test_df[\"log_peak_flux\"].max())}')\n",
    "print(f'取对数后通量最小值为 {min(train_df[\"log_peak_flux\"].min(), test_df[\"log_peak_flux\"].min())}')\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_df['standard_peak_flux'] = scaler.fit_transform(train_df['log_peak_flux'].values.reshape(-1, 1))\n",
    "test_df['standard_peak_flux'] = scaler.transform(test_df['log_peak_flux'].values.reshape(-1, 1))\n",
    "\n",
    "def denormalize_peak_flux(x, scaler):\n",
    "    return scaler.inverse_transform(x.reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "print(f'标准化后通量最大值为 {max(train_df[\"standard_peak_flux\"].max(), test_df[\"standard_peak_flux\"].max())}')\n",
    "print(f'标准化后通量最小值为 {min(train_df[\"standard_peak_flux\"].min(), test_df[\"standard_peak_flux\"].min())}')\n",
    "\n",
    "#打印标准化后的样本分布\n",
    "#25%分位数\n",
    "print(f'25%分位数: {np.percentile(train_df[\"standard_peak_flux\"], 25)}')\n",
    "print(f'75%分位数: {np.percentile(train_df[\"standard_peak_flux\"], 75)}')\n",
    "\n",
    "#去掉所有在标准化后小于-0.5的样本\n",
    "train_df = train_df[train_df['standard_peak_flux']>-1]\n",
    "test_df = test_df[test_df['standard_peak_flux']>-1]\n",
    "\n",
    "# 提取标准化后的标签\n",
    "train_labels = train_df['standard_peak_flux'].values\n",
    "test_labels = test_df['standard_peak_flux'].values\n",
    "\n",
    "\n",
    "#将处理后的训练集和验证集保存到csv文件\n",
    "train_df.to_csv(os.path.join(base_path, \"training/meta_data_processed.csv\"), index=False)\n",
    "test_df.to_csv(os.path.join(base_path, \"test/meta_data_processed.csv\"), index=False)\n",
    "\n",
    "\n",
    "'''\n",
    "#可视化样本分布\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 绘制直方图\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(train_df['peak_flux'], bins=100, alpha=0.5, label='training set')\n",
    "sns.histplot(test_df['peak_flux'], bins=100, alpha=0.5, label='validation set')\n",
    "plt.title('distrubution of peak flux')\n",
    "plt.xlabel('peak flux')\n",
    "plt.ylabel('frequency')\n",
    "plt.xlim(1.1*min(train_df['peak_flux'].min(), test_df['peak_flux'].min()), 1.1*max(train_df['peak_flux'].max(), test_df['peak_flux'].max()))\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "# 绘制直方图\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(train_df['standard_peak_flux'], bins=100, alpha=0.5, label='training set')\n",
    "sns.histplot(test_df['standard_peak_flux'], bins=100, alpha=0.5, label='validation set')\n",
    "plt.title('distrubution of peak flux(standardized)')\n",
    "plt.xlabel('standardized peak flux')\n",
    "plt.ylabel('frequency')\n",
    "plt.xlim(1.1*min(train_df['standard_peak_flux'].min(), test_df['standard_peak_flux'].min()), 1.1*max(train_df['standard_peak_flux'].max(), test_df['standard_peak_flux'].max()))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集样本数量: 7496\n",
      "样本 X 的形状: torch.Size([4, 256, 256])\n",
      "样本 y 的值: 0.9616147875785828\n",
      "测试集样本数量: 638\n",
      "样本 X 的形状: torch.Size([4, 256, 256])\n",
      "样本 y 的值: 1.1970051527023315\n",
      "验证集样本数量: 1874\n",
      "样本 X 的形状: torch.Size([4, 256, 256])\n",
      "样本 y 的值: 0.852464497089386\n"
     ]
    }
   ],
   "source": [
    "# 定义 transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]),  # Normalize grayscale image\n",
    "    #torchvision.transforms.RandomRotation(degrees = 180)\n",
    "])\n",
    "\n",
    "# 加载数据集\n",
    "train_dataset = SDOBenchmarkDataset(\n",
    "    base_path=os.path.join(base_path, \"training\"),\n",
    "    transform=transform,\n",
    "    augment=True,\n",
    "    label_func=None,\n",
    "    data_format=\"channels_last\"\n",
    ")\n",
    "\n",
    "test_dataset = SDOBenchmarkDataset(\n",
    "    base_path=os.path.join(base_path, \"test\"),\n",
    "    transform=transform,\n",
    "    augment=False,\n",
    "    label_func=None,\n",
    "    data_format=\"channels_last\"\n",
    ")\n",
    "\n",
    "# 划分训练集和验证集\n",
    "train_size = int(0.8 * len(train_dataset))  # 80% 作为训练集\n",
    "val_size = len(train_dataset) - train_size   # 20% 作为验证集\n",
    "\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "\n",
    "# Check dataset length and sample\n",
    "print(f\"训练集样本数量: {len(train_dataset)}\")\n",
    "sample_X, sample_y = train_dataset[0]\n",
    "print(f\"样本 X 的形状: {sample_X.shape}\")\n",
    "print(f\"样本 y 的值: {sample_y}\")\n",
    "\n",
    "\n",
    "print(f\"测试集样本数量: {len(test_dataset)}\")\n",
    "sample_X, sample_y = test_dataset[0]\n",
    "print(f\"样本 X 的形状: {sample_X.shape}\")\n",
    "print(f\"样本 y 的值: {sample_y}\")\n",
    "\n",
    "\n",
    "print(f\"验证集样本数量: {len(val_dataset)}\")\n",
    "sample_X, sample_y = val_dataset[0]\n",
    "print(f\"样本 X 的形状: {sample_X.shape}\")\n",
    "print(f\"样本 y 的值: {sample_y}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:1 device\n",
      "FlareCNN(\n",
      "  (conv1): Conv2d(4, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=65536, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=16, bias=True)\n",
      "  (fc3): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 定义 CNN 模型\n",
    "class FlareCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FlareCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        #self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        #self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        #self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        #self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.fc1 = nn.Linear(65536, 64)\n",
    "        self.fc2 = nn.Linear(64, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "    '''包含bn\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(self.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(self.relu(self.bn4(self.conv4(x))))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #x = x.view((-1, 64 * 128 * 128))\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = torch.exp(x)\n",
    "        return x\n",
    "    '''\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.pool(self.relu(self.conv3(x)))\n",
    "        x = self.pool(self.relu(self.conv4(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #x = x.view((-1, 64 * 128 * 128))\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = torch.exp(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "# 初始化模型\n",
    "model = FlareCNN()\n",
    "\n",
    "# 检查设备\n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))\n",
    "model.to(device)\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/aiia/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40], Step [10/235], Loss: 0.12751358300447463\n",
      "Epoch [1/40], Step [20/235], Loss: 0.11017510555684566\n",
      "Epoch [1/40], Step [30/235], Loss: 0.1188556931912899\n",
      "Epoch [1/40], Step [40/235], Loss: 0.11849648356437684\n",
      "Epoch [1/40], Step [50/235], Loss: 0.08960733860731125\n",
      "Epoch [1/40], Step [60/235], Loss: 0.13393443301320077\n",
      "Epoch [1/40], Step [70/235], Loss: 0.10682563483715057\n",
      "Epoch [1/40], Step [80/235], Loss: 0.10973731800913811\n",
      "Epoch [1/40], Step [90/235], Loss: 0.12672381103038788\n",
      "Epoch [1/40], Step [100/235], Loss: 0.09757363721728325\n",
      "Epoch [1/40], Step [110/235], Loss: 0.10302647352218627\n",
      "Epoch [1/40], Step [120/235], Loss: 0.09267238378524781\n",
      "Epoch [1/40], Step [130/235], Loss: 0.10267131328582764\n",
      "Epoch [1/40], Step [140/235], Loss: 0.09099748432636261\n",
      "Epoch [1/40], Step [150/235], Loss: 0.0919294647872448\n",
      "Epoch [1/40], Step [160/235], Loss: 0.1229618400335312\n",
      "Epoch [1/40], Step [170/235], Loss: 0.0936308152973652\n",
      "Epoch [1/40], Step [180/235], Loss: 0.1102907732129097\n",
      "Epoch [1/40], Step [190/235], Loss: 0.0979928070679307\n",
      "Epoch [1/40], Step [200/235], Loss: 0.11152798607945442\n",
      "Epoch [1/40], Step [210/235], Loss: 0.08398961424827575\n",
      "Epoch [1/40], Step [220/235], Loss: 0.08940900713205338\n",
      "Epoch [1/40], Step [230/235], Loss: 0.1082849394530058\n",
      "Epoch [1/40], Train Loss: 0.105876069207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/aiia/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40], Validation Loss: 0.113399229249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/aiia/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([18])) that is different to the input size (torch.Size([18, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/40], Step [10/235], Loss: 0.10587161183357238\n",
      "Epoch [2/40], Step [20/235], Loss: 0.10088685303926467\n",
      "Epoch [2/40], Step [30/235], Loss: 0.09729618355631828\n",
      "Epoch [2/40], Step [40/235], Loss: 0.09437319338321686\n",
      "Epoch [2/40], Step [50/235], Loss: 0.12422686889767647\n",
      "Epoch [2/40], Step [60/235], Loss: 0.08966080322861672\n",
      "Epoch [2/40], Step [70/235], Loss: 0.11616579368710518\n",
      "Epoch [2/40], Step [80/235], Loss: 0.09186070039868355\n",
      "Epoch [2/40], Step [90/235], Loss: 0.13714305087924003\n",
      "Epoch [2/40], Step [100/235], Loss: 0.10244479328393936\n",
      "Epoch [2/40], Step [110/235], Loss: 0.11791672110557556\n",
      "Epoch [2/40], Step [120/235], Loss: 0.09252794981002807\n",
      "Epoch [2/40], Step [130/235], Loss: 0.09727433398365974\n",
      "Epoch [2/40], Step [140/235], Loss: 0.11071086898446084\n",
      "Epoch [2/40], Step [150/235], Loss: 0.10707651451230049\n",
      "Epoch [2/40], Step [160/235], Loss: 0.09218686893582344\n",
      "Epoch [2/40], Step [170/235], Loss: 0.11391479447484017\n",
      "Epoch [2/40], Step [180/235], Loss: 0.1134812843054533\n",
      "Epoch [2/40], Step [190/235], Loss: 0.10209301672875881\n",
      "Epoch [2/40], Step [200/235], Loss: 0.09503918886184692\n",
      "Epoch [2/40], Step [210/235], Loss: 0.10205067992210388\n",
      "Epoch [2/40], Step [220/235], Loss: 0.09559977427124977\n",
      "Epoch [2/40], Step [230/235], Loss: 0.11581520289182663\n",
      "Epoch [2/40], Train Loss: 0.104322060253\n",
      "Epoch [2/40], Validation Loss: 0.106408354968\n",
      "Epoch [3/40], Step [10/235], Loss: 0.1166442446410656\n",
      "Epoch [3/40], Step [20/235], Loss: 0.09960136339068412\n",
      "Epoch [3/40], Step [30/235], Loss: 0.1105456106364727\n",
      "Epoch [3/40], Step [40/235], Loss: 0.09405084326863289\n",
      "Epoch [3/40], Step [50/235], Loss: 0.1037538293749094\n",
      "Epoch [3/40], Step [60/235], Loss: 0.09653519466519356\n",
      "Epoch [3/40], Step [70/235], Loss: 0.09092649258673191\n",
      "Epoch [3/40], Step [80/235], Loss: 0.10350340455770493\n",
      "Epoch [3/40], Step [90/235], Loss: 0.10396230146288872\n",
      "Epoch [3/40], Step [100/235], Loss: 0.10219120606780052\n",
      "Epoch [3/40], Step [110/235], Loss: 0.10648198574781417\n",
      "Epoch [3/40], Step [120/235], Loss: 0.10965627580881118\n",
      "Epoch [3/40], Step [130/235], Loss: 0.11128576174378395\n",
      "Epoch [3/40], Step [140/235], Loss: 0.10447116605937481\n",
      "Epoch [3/40], Step [150/235], Loss: 0.08758185766637325\n",
      "Epoch [3/40], Step [160/235], Loss: 0.10679003521800041\n",
      "Epoch [3/40], Step [170/235], Loss: 0.09119816832244396\n",
      "Epoch [3/40], Step [180/235], Loss: 0.11706243231892585\n",
      "Epoch [3/40], Step [190/235], Loss: 0.09656396061182022\n",
      "Epoch [3/40], Step [200/235], Loss: 0.1105787381529808\n",
      "Epoch [3/40], Step [210/235], Loss: 0.10375498794019222\n",
      "Epoch [3/40], Step [220/235], Loss: 0.10023995786905289\n",
      "Epoch [3/40], Step [230/235], Loss: 0.1092323511838913\n",
      "Epoch [3/40], Train Loss: 0.104913853997\n",
      "Epoch [3/40], Validation Loss: 0.129452940203\n",
      "Epoch [4/40], Step [10/235], Loss: 0.10559588670730591\n",
      "Epoch [4/40], Step [20/235], Loss: 0.1156154613941908\n",
      "Epoch [4/40], Step [30/235], Loss: 0.09793218746781349\n",
      "Epoch [4/40], Step [40/235], Loss: 0.1160177692770958\n",
      "Epoch [4/40], Step [50/235], Loss: 0.10391036495566368\n",
      "Epoch [4/40], Step [60/235], Loss: 0.1062580518424511\n",
      "Epoch [4/40], Step [70/235], Loss: 0.08677378632128238\n",
      "Epoch [4/40], Step [80/235], Loss: 0.10439589470624924\n",
      "Epoch [4/40], Step [90/235], Loss: 0.10349968373775482\n",
      "Epoch [4/40], Step [100/235], Loss: 0.0903658464550972\n",
      "Epoch [4/40], Step [110/235], Loss: 0.09750292338430881\n",
      "Epoch [4/40], Step [120/235], Loss: 0.13797711506485938\n",
      "Epoch [4/40], Step [130/235], Loss: 0.08716124333441258\n",
      "Epoch [4/40], Step [140/235], Loss: 0.09471558257937432\n",
      "Epoch [4/40], Step [150/235], Loss: 0.10110972002148629\n",
      "Epoch [4/40], Step [160/235], Loss: 0.10428937077522278\n",
      "Epoch [4/40], Step [170/235], Loss: 0.09388907104730607\n",
      "Epoch [4/40], Step [180/235], Loss: 0.09910348691046238\n",
      "Epoch [4/40], Step [190/235], Loss: 0.08419266156852245\n",
      "Epoch [4/40], Step [200/235], Loss: 0.0999033197760582\n",
      "Epoch [4/40], Step [210/235], Loss: 0.09895029962062836\n",
      "Epoch [4/40], Step [220/235], Loss: 0.10438665226101876\n",
      "Epoch [4/40], Step [230/235], Loss: 0.11768672838807107\n",
      "Epoch [4/40], Train Loss: 0.102601897114\n",
      "Epoch [4/40], Validation Loss: 0.103873835403\n",
      "Epoch [5/40], Step [10/235], Loss: 0.09664323776960373\n",
      "Epoch [5/40], Step [20/235], Loss: 0.10281281992793083\n",
      "Epoch [5/40], Step [30/235], Loss: 0.10897695198655129\n",
      "Epoch [5/40], Step [40/235], Loss: 0.1112568736076355\n",
      "Epoch [5/40], Step [50/235], Loss: 0.09713030233979225\n",
      "Epoch [5/40], Step [60/235], Loss: 0.10103400088846684\n",
      "Epoch [5/40], Step [70/235], Loss: 0.09655487760901452\n",
      "Epoch [5/40], Step [80/235], Loss: 0.0963860061019659\n",
      "Epoch [5/40], Step [90/235], Loss: 0.08564995974302292\n",
      "Epoch [5/40], Step [100/235], Loss: 0.09336153902113438\n",
      "Epoch [5/40], Step [110/235], Loss: 0.10104824788868427\n",
      "Epoch [5/40], Step [120/235], Loss: 0.10055343955755233\n",
      "Epoch [5/40], Step [130/235], Loss: 0.09201640710234642\n",
      "Epoch [5/40], Step [140/235], Loss: 0.11811366453766822\n",
      "Epoch [5/40], Step [150/235], Loss: 0.10881344377994537\n",
      "Epoch [5/40], Step [160/235], Loss: 0.09478012919425964\n",
      "Epoch [5/40], Step [170/235], Loss: 0.10859716013073921\n",
      "Epoch [5/40], Step [180/235], Loss: 0.10913717746734619\n",
      "Epoch [5/40], Step [190/235], Loss: 0.1071236938238144\n",
      "Epoch [5/40], Step [200/235], Loss: 0.09169009774923324\n",
      "Epoch [5/40], Step [210/235], Loss: 0.09262853711843491\n",
      "Epoch [5/40], Step [220/235], Loss: 0.10188999474048614\n",
      "Epoch [5/40], Step [230/235], Loss: 0.11893677338957787\n",
      "Epoch [5/40], Train Loss: 0.101305503985\n",
      "Epoch [5/40], Validation Loss: 0.103896349795\n",
      "Epoch [6/40], Step [10/235], Loss: 0.11799216419458389\n",
      "Epoch [6/40], Step [20/235], Loss: 0.11323836594820022\n",
      "Epoch [6/40], Step [30/235], Loss: 0.09448117762804031\n",
      "Epoch [6/40], Step [40/235], Loss: 0.10620456412434578\n",
      "Epoch [6/40], Step [50/235], Loss: 0.0883597880601883\n",
      "Epoch [6/40], Step [60/235], Loss: 0.09104409031569957\n",
      "Epoch [6/40], Step [70/235], Loss: 0.09875553101301193\n",
      "Epoch [6/40], Step [80/235], Loss: 0.08553303368389606\n",
      "Epoch [6/40], Step [90/235], Loss: 0.11535587012767792\n",
      "Epoch [6/40], Step [100/235], Loss: 0.12806689962744713\n",
      "Epoch [6/40], Step [110/235], Loss: 0.10039326176047325\n",
      "Epoch [6/40], Step [120/235], Loss: 0.10561103336513042\n",
      "Epoch [6/40], Step [130/235], Loss: 0.09107394367456437\n",
      "Epoch [6/40], Step [140/235], Loss: 0.10221365541219711\n",
      "Epoch [6/40], Step [150/235], Loss: 0.09145928025245667\n",
      "Epoch [6/40], Step [160/235], Loss: 0.0858800832182169\n",
      "Epoch [6/40], Step [170/235], Loss: 0.09136104136705399\n",
      "Epoch [6/40], Step [180/235], Loss: 0.09484037086367607\n",
      "Epoch [6/40], Step [190/235], Loss: 0.10804676935076714\n",
      "Epoch [6/40], Step [200/235], Loss: 0.10934427082538604\n",
      "Epoch [6/40], Step [210/235], Loss: 0.11566798314452172\n",
      "Epoch [6/40], Step [220/235], Loss: 0.09583216086030007\n",
      "Epoch [6/40], Step [230/235], Loss: 0.09056193381547928\n",
      "Epoch [6/40], Train Loss: 0.101238812062\n",
      "Epoch [6/40], Validation Loss: 0.104464038323\n",
      "Epoch [7/40], Step [10/235], Loss: 0.1068619929254055\n",
      "Epoch [7/40], Step [20/235], Loss: 0.09227236583828927\n",
      "Epoch [7/40], Step [30/235], Loss: 0.09943758361041546\n",
      "Epoch [7/40], Step [40/235], Loss: 0.10041579827666283\n",
      "Epoch [7/40], Step [50/235], Loss: 0.09846537485718727\n",
      "Epoch [7/40], Step [60/235], Loss: 0.08893166743218898\n",
      "Epoch [7/40], Step [70/235], Loss: 0.09475752711296082\n",
      "Epoch [7/40], Step [80/235], Loss: 0.10310249924659728\n",
      "Epoch [7/40], Step [90/235], Loss: 0.09246830269694328\n",
      "Epoch [7/40], Step [100/235], Loss: 0.1030187375843525\n",
      "Epoch [7/40], Step [110/235], Loss: 0.09705624133348464\n",
      "Epoch [7/40], Step [120/235], Loss: 0.08467711582779884\n",
      "Epoch [7/40], Step [130/235], Loss: 0.10178232342004775\n",
      "Epoch [7/40], Step [140/235], Loss: 0.11889861300587654\n",
      "Epoch [7/40], Step [150/235], Loss: 0.11595632843673229\n",
      "Epoch [7/40], Step [160/235], Loss: 0.09392120018601417\n",
      "Epoch [7/40], Step [170/235], Loss: 0.11973345279693604\n",
      "Epoch [7/40], Step [180/235], Loss: 0.1021748274564743\n",
      "Epoch [7/40], Step [190/235], Loss: 0.10447917729616166\n",
      "Epoch [7/40], Step [200/235], Loss: 0.10531544908881188\n",
      "Epoch [7/40], Step [210/235], Loss: 0.10886038467288017\n",
      "Epoch [7/40], Step [220/235], Loss: 0.09796315878629684\n",
      "Epoch [7/40], Step [230/235], Loss: 0.09147095754742622\n",
      "Epoch [7/40], Train Loss: 0.101092264659\n",
      "Epoch [7/40], Validation Loss: 0.105050703858\n",
      "Epoch [8/40], Step [10/235], Loss: 0.09478227496147155\n",
      "Epoch [8/40], Step [20/235], Loss: 0.10135892629623414\n",
      "Epoch [8/40], Step [30/235], Loss: 0.13284438624978065\n",
      "Epoch [8/40], Step [40/235], Loss: 0.0957657553255558\n",
      "Epoch [8/40], Step [50/235], Loss: 0.1025775544345379\n",
      "Epoch [8/40], Step [60/235], Loss: 0.09683000296354294\n",
      "Epoch [8/40], Step [70/235], Loss: 0.1019772469997406\n",
      "Epoch [8/40], Step [80/235], Loss: 0.11038329750299454\n",
      "Epoch [8/40], Step [90/235], Loss: 0.09280994161963463\n",
      "Epoch [8/40], Step [100/235], Loss: 0.08537725061178207\n",
      "Epoch [8/40], Step [110/235], Loss: 0.09753229692578316\n",
      "Epoch [8/40], Step [120/235], Loss: 0.09345005564391613\n",
      "Epoch [8/40], Step [130/235], Loss: 0.1124015673995018\n",
      "Epoch [8/40], Step [140/235], Loss: 0.09989806190133095\n",
      "Epoch [8/40], Step [150/235], Loss: 0.10895193889737129\n",
      "Epoch [8/40], Step [160/235], Loss: 0.10927229784429074\n",
      "Epoch [8/40], Step [170/235], Loss: 0.1051262840628624\n",
      "Epoch [8/40], Step [180/235], Loss: 0.08687628395855426\n",
      "Epoch [8/40], Step [190/235], Loss: 0.10525712631642818\n",
      "Epoch [8/40], Step [200/235], Loss: 0.10707011520862579\n",
      "Epoch [8/40], Step [210/235], Loss: 0.09549161940813064\n",
      "Epoch [8/40], Step [220/235], Loss: 0.09028340950608253\n",
      "Epoch [8/40], Step [230/235], Loss: 0.11235505566000939\n",
      "Epoch [8/40], Train Loss: 0.101842796692\n",
      "Epoch [8/40], Validation Loss: 0.103765548601\n",
      "Epoch [9/40], Step [10/235], Loss: 0.10526571348309517\n",
      "Epoch [9/40], Step [20/235], Loss: 0.1028168335556984\n",
      "Epoch [9/40], Step [30/235], Loss: 0.1225206770002842\n",
      "Epoch [9/40], Step [40/235], Loss: 0.09347926825284958\n",
      "Epoch [9/40], Step [50/235], Loss: 0.10500481240451336\n",
      "Epoch [9/40], Step [60/235], Loss: 0.09794645830988884\n",
      "Epoch [9/40], Step [70/235], Loss: 0.0998126171529293\n",
      "Epoch [9/40], Step [80/235], Loss: 0.08578910641372203\n",
      "Epoch [9/40], Step [90/235], Loss: 0.10300296433269977\n",
      "Epoch [9/40], Step [100/235], Loss: 0.10984190851449967\n",
      "Epoch [9/40], Step [110/235], Loss: 0.11548463925719261\n",
      "Epoch [9/40], Step [120/235], Loss: 0.09553033486008644\n",
      "Epoch [9/40], Step [130/235], Loss: 0.09775399900972843\n",
      "Epoch [9/40], Step [140/235], Loss: 0.11818569153547287\n",
      "Epoch [9/40], Step [150/235], Loss: 0.10386460199952126\n",
      "Epoch [9/40], Step [160/235], Loss: 0.10037161409854889\n",
      "Epoch [9/40], Step [170/235], Loss: 0.09081634394824505\n",
      "Epoch [9/40], Step [180/235], Loss: 0.08517327085137368\n",
      "Epoch [9/40], Step [190/235], Loss: 0.09094296656548977\n",
      "Epoch [9/40], Step [200/235], Loss: 0.09147757031023503\n",
      "Epoch [9/40], Step [210/235], Loss: 0.10905442610383034\n",
      "Epoch [9/40], Step [220/235], Loss: 0.1050191842019558\n",
      "Epoch [9/40], Step [230/235], Loss: 0.1063715547323227\n",
      "Epoch [9/40], Train Loss: 0.101089072957\n",
      "Epoch [9/40], Validation Loss: 0.103672976400\n",
      "Epoch [10/40], Step [10/235], Loss: 0.1192884936928749\n",
      "Epoch [10/40], Step [20/235], Loss: 0.09843189269304276\n",
      "Epoch [10/40], Step [30/235], Loss: 0.08710751570761203\n",
      "Epoch [10/40], Step [40/235], Loss: 0.10462765544652938\n",
      "Epoch [10/40], Step [50/235], Loss: 0.09716199785470962\n",
      "Epoch [10/40], Step [60/235], Loss: 0.11257116347551346\n",
      "Epoch [10/40], Step [70/235], Loss: 0.08505684174597264\n",
      "Epoch [10/40], Step [80/235], Loss: 0.10199669077992439\n",
      "Epoch [10/40], Step [90/235], Loss: 0.08758768811821938\n",
      "Epoch [10/40], Step [100/235], Loss: 0.09925728477537632\n",
      "Epoch [10/40], Step [110/235], Loss: 0.10131673812866211\n",
      "Epoch [10/40], Step [120/235], Loss: 0.09725010432302952\n",
      "Epoch [10/40], Step [130/235], Loss: 0.11281031221151352\n",
      "Epoch [10/40], Step [140/235], Loss: 0.10741021111607552\n",
      "Epoch [10/40], Step [150/235], Loss: 0.09514335319399833\n",
      "Epoch [10/40], Step [160/235], Loss: 0.11576894372701645\n",
      "Epoch [10/40], Step [170/235], Loss: 0.10228929892182351\n",
      "Epoch [10/40], Step [180/235], Loss: 0.10373726412653923\n",
      "Epoch [10/40], Step [190/235], Loss: 0.112458997964859\n",
      "Epoch [10/40], Step [200/235], Loss: 0.11336509063839913\n",
      "Epoch [10/40], Step [210/235], Loss: 0.09090644717216492\n",
      "Epoch [10/40], Step [220/235], Loss: 0.09109125807881355\n",
      "Epoch [10/40], Step [230/235], Loss: 0.08942029327154159\n",
      "Epoch [10/40], Train Loss: 0.101433056815\n",
      "Epoch [10/40], Validation Loss: 0.104051733232\n",
      "Epoch [11/40], Step [10/235], Loss: 0.0992568589746952\n",
      "Epoch [11/40], Step [20/235], Loss: 0.11426971480250359\n",
      "Epoch [11/40], Step [30/235], Loss: 0.09610574245452881\n",
      "Epoch [11/40], Step [40/235], Loss: 0.10663768425583839\n",
      "Epoch [11/40], Step [50/235], Loss: 0.09363836199045181\n",
      "Epoch [11/40], Step [60/235], Loss: 0.10209012925624847\n",
      "Epoch [11/40], Step [70/235], Loss: 0.08772917240858077\n",
      "Epoch [11/40], Step [80/235], Loss: 0.12720081731677055\n",
      "Epoch [11/40], Step [90/235], Loss: 0.10611646920442581\n",
      "Epoch [11/40], Step [100/235], Loss: 0.08041624389588833\n",
      "Epoch [11/40], Step [110/235], Loss: 0.1198061864823103\n",
      "Epoch [11/40], Step [120/235], Loss: 0.11592690423130989\n",
      "Epoch [11/40], Step [130/235], Loss: 0.09163824245333671\n",
      "Epoch [11/40], Step [140/235], Loss: 0.0864489208906889\n",
      "Epoch [11/40], Step [150/235], Loss: 0.08887580558657646\n",
      "Epoch [11/40], Step [160/235], Loss: 0.08450025655329227\n",
      "Epoch [11/40], Step [170/235], Loss: 0.09791613891720771\n",
      "Epoch [11/40], Step [180/235], Loss: 0.11553477421402931\n",
      "Epoch [11/40], Step [190/235], Loss: 0.10437229722738266\n",
      "Epoch [11/40], Step [200/235], Loss: 0.09234197624027729\n",
      "Epoch [11/40], Step [210/235], Loss: 0.10698480010032654\n",
      "Epoch [11/40], Step [220/235], Loss: 0.1030686378479004\n",
      "Epoch [11/40], Step [230/235], Loss: 0.11412766650319099\n",
      "Epoch [11/40], Train Loss: 0.101113457169\n",
      "Epoch [11/40], Validation Loss: 0.103625540764\n",
      "Epoch [12/40], Step [10/235], Loss: 0.10262126848101616\n",
      "Epoch [12/40], Step [20/235], Loss: 0.11731582581996917\n",
      "Epoch [12/40], Step [30/235], Loss: 0.10361864939332008\n",
      "Epoch [12/40], Step [40/235], Loss: 0.09245954528450966\n",
      "Epoch [12/40], Step [50/235], Loss: 0.09403111115097999\n",
      "Epoch [12/40], Step [60/235], Loss: 0.10553981438279152\n",
      "Epoch [12/40], Step [70/235], Loss: 0.09966887310147285\n",
      "Epoch [12/40], Step [80/235], Loss: 0.0975849986076355\n",
      "Epoch [12/40], Step [90/235], Loss: 0.10142053067684173\n",
      "Epoch [12/40], Step [100/235], Loss: 0.1050200391560793\n",
      "Epoch [12/40], Step [110/235], Loss: 0.10017914399504661\n",
      "Epoch [12/40], Step [120/235], Loss: 0.08689127787947655\n",
      "Epoch [12/40], Step [130/235], Loss: 0.09464686885476112\n",
      "Epoch [12/40], Step [140/235], Loss: 0.08836991712450981\n",
      "Epoch [12/40], Step [150/235], Loss: 0.10756950043141841\n",
      "Epoch [12/40], Step [160/235], Loss: 0.08159676305949688\n",
      "Epoch [12/40], Step [170/235], Loss: 0.09869428724050522\n",
      "Epoch [12/40], Step [180/235], Loss: 0.11580727696418762\n",
      "Epoch [12/40], Step [190/235], Loss: 0.11517716124653817\n",
      "Epoch [12/40], Step [200/235], Loss: 0.100967076420784\n",
      "Epoch [12/40], Step [210/235], Loss: 0.11091710031032562\n",
      "Epoch [12/40], Step [220/235], Loss: 0.106028251349926\n",
      "Epoch [12/40], Step [230/235], Loss: 0.1010562539100647\n",
      "Epoch [12/40], Train Loss: 0.101633553540\n",
      "Epoch [12/40], Validation Loss: 0.103568312728\n",
      "Epoch [13/40], Step [10/235], Loss: 0.11469916850328446\n",
      "Epoch [13/40], Step [20/235], Loss: 0.09074662141501903\n",
      "Epoch [13/40], Step [30/235], Loss: 0.11132536083459854\n",
      "Epoch [13/40], Step [40/235], Loss: 0.11732001826167107\n",
      "Epoch [13/40], Step [50/235], Loss: 0.092213224619627\n",
      "Epoch [13/40], Step [60/235], Loss: 0.10695351064205169\n",
      "Epoch [13/40], Step [70/235], Loss: 0.08490131720900536\n",
      "Epoch [13/40], Step [80/235], Loss: 0.09721902348101139\n",
      "Epoch [13/40], Step [90/235], Loss: 0.09803378805518151\n",
      "Epoch [13/40], Step [100/235], Loss: 0.11429484710097312\n",
      "Epoch [13/40], Step [110/235], Loss: 0.08731529898941517\n",
      "Epoch [13/40], Step [120/235], Loss: 0.10738987773656845\n",
      "Epoch [13/40], Step [130/235], Loss: 0.0959343034774065\n",
      "Epoch [13/40], Step [140/235], Loss: 0.09502487331628799\n",
      "Epoch [13/40], Step [150/235], Loss: 0.10435090139508248\n",
      "Epoch [13/40], Step [160/235], Loss: 0.10180442072451115\n",
      "Epoch [13/40], Step [170/235], Loss: 0.10872735977172851\n",
      "Epoch [13/40], Step [180/235], Loss: 0.09964601770043373\n",
      "Epoch [13/40], Step [190/235], Loss: 0.10765573158860206\n",
      "Epoch [13/40], Step [200/235], Loss: 0.10709342136979103\n",
      "Epoch [13/40], Step [210/235], Loss: 0.0987087495625019\n",
      "Epoch [13/40], Step [220/235], Loss: 0.1041036456823349\n",
      "Epoch [13/40], Step [230/235], Loss: 0.082447724416852\n",
      "Epoch [13/40], Train Loss: 0.101240831170\n",
      "Epoch [13/40], Validation Loss: 0.103529123684\n",
      "Epoch [14/40], Step [10/235], Loss: 0.09918198958039284\n",
      "Epoch [14/40], Step [20/235], Loss: 0.09732076674699783\n",
      "Epoch [14/40], Step [30/235], Loss: 0.0943475641310215\n",
      "Epoch [14/40], Step [40/235], Loss: 0.1096083588898182\n",
      "Epoch [14/40], Step [50/235], Loss: 0.10893235132098197\n",
      "Epoch [14/40], Step [60/235], Loss: 0.09311730787158012\n",
      "Epoch [14/40], Step [70/235], Loss: 0.09715427458286285\n",
      "Epoch [14/40], Step [80/235], Loss: 0.12147450968623161\n",
      "Epoch [14/40], Step [90/235], Loss: 0.1009157307446003\n",
      "Epoch [14/40], Step [100/235], Loss: 0.0872148796916008\n",
      "Epoch [14/40], Step [110/235], Loss: 0.10148636102676392\n",
      "Epoch [14/40], Step [120/235], Loss: 0.10033252313733101\n",
      "Epoch [14/40], Step [130/235], Loss: 0.09605165757238865\n",
      "Epoch [14/40], Step [140/235], Loss: 0.09919929876923561\n",
      "Epoch [14/40], Step [150/235], Loss: 0.11912707760930061\n",
      "Epoch [14/40], Step [160/235], Loss: 0.1011730745434761\n",
      "Epoch [14/40], Step [170/235], Loss: 0.09304682724177837\n",
      "Epoch [14/40], Step [180/235], Loss: 0.09761459678411484\n",
      "Epoch [14/40], Step [190/235], Loss: 0.0976141169667244\n",
      "Epoch [14/40], Step [200/235], Loss: 0.10434412136673928\n",
      "Epoch [14/40], Step [210/235], Loss: 0.09189798347651959\n",
      "Epoch [14/40], Step [220/235], Loss: 0.11453418433666229\n",
      "Epoch [14/40], Step [230/235], Loss: 0.10570461302995682\n",
      "Epoch [14/40], Train Loss: 0.100969850581\n",
      "Epoch [14/40], Validation Loss: 0.103753837867\n",
      "Epoch [15/40], Step [10/235], Loss: 0.10207699984312057\n",
      "Epoch [15/40], Step [20/235], Loss: 0.09158883839845658\n",
      "Epoch [15/40], Step [30/235], Loss: 0.1059768408536911\n",
      "Epoch [15/40], Step [40/235], Loss: 0.08851404413580895\n",
      "Epoch [15/40], Step [50/235], Loss: 0.07700141109526157\n",
      "Epoch [15/40], Step [60/235], Loss: 0.09141140095889569\n",
      "Epoch [15/40], Step [70/235], Loss: 0.09596981480717659\n",
      "Epoch [15/40], Step [80/235], Loss: 0.1000890664756298\n",
      "Epoch [15/40], Step [90/235], Loss: 0.10300659686326981\n",
      "Epoch [15/40], Step [100/235], Loss: 0.11020651683211327\n",
      "Epoch [15/40], Step [110/235], Loss: 0.11831524521112442\n",
      "Epoch [15/40], Step [120/235], Loss: 0.09640796408057213\n",
      "Epoch [15/40], Step [130/235], Loss: 0.10367836505174637\n",
      "Epoch [15/40], Step [140/235], Loss: 0.09464602284133435\n",
      "Epoch [15/40], Step [150/235], Loss: 0.09156300686299801\n",
      "Epoch [15/40], Step [160/235], Loss: 0.10972331315279008\n",
      "Epoch [15/40], Step [170/235], Loss: 0.10578155741095543\n",
      "Epoch [15/40], Step [180/235], Loss: 0.11972989067435265\n",
      "Epoch [15/40], Step [190/235], Loss: 0.1041979119181633\n",
      "Epoch [15/40], Step [200/235], Loss: 0.10417265221476554\n",
      "Epoch [15/40], Step [210/235], Loss: 0.11363969892263412\n",
      "Epoch [15/40], Step [220/235], Loss: 0.09916896931827068\n",
      "Epoch [15/40], Step [230/235], Loss: 0.10186659693717956\n",
      "Epoch [15/40], Train Loss: 0.101090706061\n",
      "Epoch [15/40], Validation Loss: 0.104505266931\n",
      "Epoch [16/40], Step [10/235], Loss: 0.09275994747877121\n",
      "Epoch [16/40], Step [20/235], Loss: 0.0918734047561884\n",
      "Epoch [16/40], Step [30/235], Loss: 0.10426418110728264\n",
      "Epoch [16/40], Step [40/235], Loss: 0.09167407751083374\n",
      "Epoch [16/40], Step [50/235], Loss: 0.09351764544844628\n",
      "Epoch [16/40], Step [60/235], Loss: 0.0874435544013977\n",
      "Epoch [16/40], Step [70/235], Loss: 0.10949343144893646\n",
      "Epoch [16/40], Step [80/235], Loss: 0.10024836808443069\n",
      "Epoch [16/40], Step [90/235], Loss: 0.10698556751012803\n",
      "Epoch [16/40], Step [100/235], Loss: 0.11627167090773582\n",
      "Epoch [16/40], Step [110/235], Loss: 0.10390489250421524\n",
      "Epoch [16/40], Step [120/235], Loss: 0.10714168436825275\n",
      "Epoch [16/40], Step [130/235], Loss: 0.09902290105819703\n",
      "Epoch [16/40], Step [140/235], Loss: 0.10477167591452599\n",
      "Epoch [16/40], Step [150/235], Loss: 0.10995653420686721\n",
      "Epoch [16/40], Step [160/235], Loss: 0.10183117054402828\n",
      "Epoch [16/40], Step [170/235], Loss: 0.09527864530682564\n",
      "Epoch [16/40], Step [180/235], Loss: 0.10620062574744224\n",
      "Epoch [16/40], Step [190/235], Loss: 0.08665004894137382\n",
      "Epoch [16/40], Step [200/235], Loss: 0.10588015653192998\n",
      "Epoch [16/40], Step [210/235], Loss: 0.10818763934075833\n",
      "Epoch [16/40], Step [220/235], Loss: 0.10466076880693435\n",
      "Epoch [16/40], Step [230/235], Loss: 0.10271175429224969\n",
      "Epoch [16/40], Train Loss: 0.100911957168\n",
      "Epoch [16/40], Validation Loss: 0.103685534417\n",
      "Epoch [17/40], Step [10/235], Loss: 0.10680411532521247\n",
      "Epoch [17/40], Step [20/235], Loss: 0.1029619187116623\n",
      "Epoch [17/40], Step [30/235], Loss: 0.10676725879311562\n",
      "Epoch [17/40], Step [40/235], Loss: 0.09570954069495201\n",
      "Epoch [17/40], Step [50/235], Loss: 0.10613844469189644\n",
      "Epoch [17/40], Step [60/235], Loss: 0.1059257484972477\n",
      "Epoch [17/40], Step [70/235], Loss: 0.11167424842715264\n",
      "Epoch [17/40], Step [80/235], Loss: 0.09924522563815116\n",
      "Epoch [17/40], Step [90/235], Loss: 0.100251005589962\n",
      "Epoch [17/40], Step [100/235], Loss: 0.09213959872722625\n",
      "Epoch [17/40], Step [110/235], Loss: 0.09986135922372341\n",
      "Epoch [17/40], Step [120/235], Loss: 0.08827331885695458\n",
      "Epoch [17/40], Step [130/235], Loss: 0.09049888402223587\n",
      "Epoch [17/40], Step [140/235], Loss: 0.0948838748037815\n",
      "Epoch [17/40], Step [150/235], Loss: 0.1025901485234499\n",
      "Epoch [17/40], Step [160/235], Loss: 0.1009149920195341\n",
      "Epoch [17/40], Step [170/235], Loss: 0.11857677176594734\n",
      "Epoch [17/40], Step [180/235], Loss: 0.11237511858344078\n",
      "Epoch [17/40], Step [190/235], Loss: 0.09124262630939484\n",
      "Epoch [17/40], Step [200/235], Loss: 0.10814162008464337\n",
      "Epoch [17/40], Step [210/235], Loss: 0.10792976692318916\n",
      "Epoch [17/40], Step [220/235], Loss: 0.09150839745998382\n",
      "Epoch [17/40], Step [230/235], Loss: 0.09037788510322571\n",
      "Epoch [17/40], Train Loss: 0.101235284609\n",
      "Epoch [17/40], Validation Loss: 0.104250063709\n",
      "Epoch [18/40], Step [10/235], Loss: 0.09896403476595879\n",
      "Epoch [18/40], Step [20/235], Loss: 0.11473428532481193\n",
      "Epoch [18/40], Step [30/235], Loss: 0.10397897064685821\n",
      "Epoch [18/40], Step [40/235], Loss: 0.10274286270141601\n",
      "Epoch [18/40], Step [50/235], Loss: 0.10851913169026375\n",
      "Epoch [18/40], Step [60/235], Loss: 0.09166365191340446\n",
      "Epoch [18/40], Step [70/235], Loss: 0.10041405335068702\n",
      "Epoch [18/40], Step [80/235], Loss: 0.09371804818511009\n",
      "Epoch [18/40], Step [90/235], Loss: 0.1010356243699789\n",
      "Epoch [18/40], Step [100/235], Loss: 0.09834367781877518\n",
      "Epoch [18/40], Step [110/235], Loss: 0.09922464825212955\n",
      "Epoch [18/40], Step [120/235], Loss: 0.09804714471101761\n",
      "Epoch [18/40], Step [130/235], Loss: 0.0940278086811304\n",
      "Epoch [18/40], Step [140/235], Loss: 0.09793685898184776\n",
      "Epoch [18/40], Step [150/235], Loss: 0.11057107970118522\n",
      "Epoch [18/40], Step [160/235], Loss: 0.08963451907038689\n",
      "Epoch [18/40], Step [170/235], Loss: 0.0939420446753502\n",
      "Epoch [18/40], Step [180/235], Loss: 0.09469288289546966\n",
      "Epoch [18/40], Step [190/235], Loss: 0.10700772181153298\n",
      "Epoch [18/40], Step [200/235], Loss: 0.12342035844922065\n",
      "Epoch [18/40], Step [210/235], Loss: 0.10361054576933384\n",
      "Epoch [18/40], Step [220/235], Loss: 0.08412277102470397\n",
      "Epoch [18/40], Step [230/235], Loss: 0.1144554227590561\n",
      "Epoch [18/40], Train Loss: 0.101067211305\n",
      "Epoch [18/40], Validation Loss: 0.103492639075\n",
      "Epoch [19/40], Step [10/235], Loss: 0.10976308509707451\n",
      "Epoch [19/40], Step [20/235], Loss: 0.09555917382240295\n",
      "Epoch [19/40], Step [30/235], Loss: 0.10095753744244576\n",
      "Epoch [19/40], Step [40/235], Loss: 0.09646104276180267\n",
      "Epoch [19/40], Step [50/235], Loss: 0.10081074610352517\n",
      "Epoch [19/40], Step [60/235], Loss: 0.10309604406356812\n",
      "Epoch [19/40], Step [70/235], Loss: 0.09478041790425777\n",
      "Epoch [19/40], Step [80/235], Loss: 0.10697941072285175\n",
      "Epoch [19/40], Step [90/235], Loss: 0.08821405284106731\n",
      "Epoch [19/40], Step [100/235], Loss: 0.11128726862370968\n",
      "Epoch [19/40], Step [110/235], Loss: 0.08591572120785713\n",
      "Epoch [19/40], Step [120/235], Loss: 0.11441048160195351\n",
      "Epoch [19/40], Step [130/235], Loss: 0.09323336333036422\n",
      "Epoch [19/40], Step [140/235], Loss: 0.1122575931251049\n",
      "Epoch [19/40], Step [150/235], Loss: 0.0856869526207447\n",
      "Epoch [19/40], Step [160/235], Loss: 0.11300133988261223\n",
      "Epoch [19/40], Step [170/235], Loss: 0.10510808229446411\n",
      "Epoch [19/40], Step [180/235], Loss: 0.09767463430762291\n",
      "Epoch [19/40], Step [190/235], Loss: 0.09938383623957633\n",
      "Epoch [19/40], Step [200/235], Loss: 0.10619166493415833\n",
      "Epoch [19/40], Step [210/235], Loss: 0.10247666649520397\n",
      "Epoch [19/40], Step [220/235], Loss: 0.10241082236170769\n",
      "Epoch [19/40], Step [230/235], Loss: 0.10329043120145798\n",
      "Epoch [19/40], Train Loss: 0.101252174330\n",
      "Epoch [19/40], Validation Loss: 0.104678348338\n",
      "Epoch [20/40], Step [10/235], Loss: 0.09764929637312889\n",
      "Epoch [20/40], Step [20/235], Loss: 0.09053061492741107\n",
      "Epoch [20/40], Step [30/235], Loss: 0.09419883750379085\n",
      "Epoch [20/40], Step [40/235], Loss: 0.10415841042995452\n",
      "Epoch [20/40], Step [50/235], Loss: 0.10306954458355903\n",
      "Epoch [20/40], Step [60/235], Loss: 0.09476254135370255\n",
      "Epoch [20/40], Step [70/235], Loss: 0.112005303055048\n",
      "Epoch [20/40], Step [80/235], Loss: 0.08445044606924057\n",
      "Epoch [20/40], Step [90/235], Loss: 0.09924576878547668\n",
      "Epoch [20/40], Step [100/235], Loss: 0.1018693633377552\n",
      "Epoch [20/40], Step [110/235], Loss: 0.10869957134127617\n",
      "Epoch [20/40], Step [120/235], Loss: 0.11985078230500221\n",
      "Epoch [20/40], Step [130/235], Loss: 0.08604529686272144\n",
      "Epoch [20/40], Step [140/235], Loss: 0.07996002361178398\n",
      "Epoch [20/40], Step [150/235], Loss: 0.1044413261115551\n",
      "Epoch [20/40], Step [160/235], Loss: 0.10553464889526368\n",
      "Epoch [20/40], Step [170/235], Loss: 0.11624412015080451\n",
      "Epoch [20/40], Step [180/235], Loss: 0.09057577662169933\n",
      "Epoch [20/40], Step [190/235], Loss: 0.09294104278087616\n",
      "Epoch [20/40], Step [200/235], Loss: 0.09460166990756988\n",
      "Epoch [20/40], Step [210/235], Loss: 0.11520883180201054\n",
      "Epoch [20/40], Step [220/235], Loss: 0.10233390405774116\n",
      "Epoch [20/40], Step [230/235], Loss: 0.12854189053177834\n",
      "Epoch [20/40], Train Loss: 0.101049960785\n",
      "Epoch [20/40], Validation Loss: 0.105570664219\n",
      "Epoch [21/40], Step [10/235], Loss: 0.09902137666940689\n",
      "Epoch [21/40], Step [20/235], Loss: 0.10045332312583924\n",
      "Epoch [21/40], Step [30/235], Loss: 0.10091239884495735\n",
      "Epoch [21/40], Step [40/235], Loss: 0.11300670355558395\n",
      "Epoch [21/40], Step [50/235], Loss: 0.10865664929151535\n",
      "Epoch [21/40], Step [60/235], Loss: 0.11371545195579529\n",
      "Epoch [21/40], Step [70/235], Loss: 0.10578086301684379\n",
      "Epoch [21/40], Step [80/235], Loss: 0.09917182698845864\n",
      "Epoch [21/40], Step [90/235], Loss: 0.1047387771308422\n",
      "Epoch [21/40], Step [100/235], Loss: 0.10699839554727078\n",
      "Epoch [21/40], Step [110/235], Loss: 0.10008035078644753\n",
      "Epoch [21/40], Step [120/235], Loss: 0.10049709230661392\n",
      "Epoch [21/40], Step [130/235], Loss: 0.09803985431790352\n",
      "Epoch [21/40], Step [140/235], Loss: 0.09989028610289097\n",
      "Epoch [21/40], Step [150/235], Loss: 0.11377881690859795\n",
      "Epoch [21/40], Step [160/235], Loss: 0.10409966558218002\n",
      "Epoch [21/40], Step [170/235], Loss: 0.10471109300851822\n",
      "Epoch [21/40], Step [180/235], Loss: 0.0884879432618618\n",
      "Epoch [21/40], Step [190/235], Loss: 0.09853741824626923\n",
      "Epoch [21/40], Step [200/235], Loss: 0.08148110508918763\n",
      "Epoch [21/40], Step [210/235], Loss: 0.08161947280168533\n",
      "Epoch [21/40], Step [220/235], Loss: 0.10096301436424256\n",
      "Epoch [21/40], Step [230/235], Loss: 0.10063832104206086\n",
      "Epoch [21/40], Train Loss: 0.101325815282\n",
      "Epoch [21/40], Validation Loss: 0.103462682437\n",
      "Epoch [22/40], Step [10/235], Loss: 0.09265055805444718\n",
      "Epoch [22/40], Step [20/235], Loss: 0.13376276269555093\n",
      "Epoch [22/40], Step [30/235], Loss: 0.10554869472980499\n",
      "Epoch [22/40], Step [40/235], Loss: 0.08374594673514366\n",
      "Epoch [22/40], Step [50/235], Loss: 0.11587018519639969\n",
      "Epoch [22/40], Step [60/235], Loss: 0.10257819592952729\n",
      "Epoch [22/40], Step [70/235], Loss: 0.11663203462958335\n",
      "Epoch [22/40], Step [80/235], Loss: 0.09071381911635398\n",
      "Epoch [22/40], Step [90/235], Loss: 0.08827631585299969\n",
      "Epoch [22/40], Step [100/235], Loss: 0.09600258097052575\n",
      "Epoch [22/40], Step [110/235], Loss: 0.10833111181855201\n",
      "Epoch [22/40], Step [120/235], Loss: 0.10806913077831268\n",
      "Epoch [22/40], Step [130/235], Loss: 0.07560231648385525\n",
      "Epoch [22/40], Step [140/235], Loss: 0.09154582843184471\n",
      "Epoch [22/40], Step [150/235], Loss: 0.09722596481442451\n",
      "Epoch [22/40], Step [160/235], Loss: 0.10000112503767014\n",
      "Epoch [22/40], Step [170/235], Loss: 0.09448767676949502\n",
      "Epoch [22/40], Step [180/235], Loss: 0.08376359194517136\n",
      "Epoch [22/40], Step [190/235], Loss: 0.10704936608672141\n",
      "Epoch [22/40], Step [200/235], Loss: 0.10298678502440453\n",
      "Epoch [22/40], Step [210/235], Loss: 0.1061246432363987\n",
      "Epoch [22/40], Step [220/235], Loss: 0.10678135082125664\n",
      "Epoch [22/40], Step [230/235], Loss: 0.10850530192255974\n",
      "Epoch [22/40], Train Loss: 0.100772967491\n",
      "Epoch [22/40], Validation Loss: 0.103394070924\n",
      "Epoch [23/40], Step [10/235], Loss: 0.10511247366666794\n",
      "Epoch [23/40], Step [20/235], Loss: 0.11101815477013588\n",
      "Epoch [23/40], Step [30/235], Loss: 0.10743254572153091\n",
      "Epoch [23/40], Step [40/235], Loss: 0.10893487334251403\n",
      "Epoch [23/40], Step [50/235], Loss: 0.0957263357937336\n",
      "Epoch [23/40], Step [60/235], Loss: 0.10739890486001968\n",
      "Epoch [23/40], Step [70/235], Loss: 0.10513935089111329\n",
      "Epoch [23/40], Step [80/235], Loss: 0.1074626374989748\n",
      "Epoch [23/40], Step [90/235], Loss: 0.08354569748044013\n",
      "Epoch [23/40], Step [100/235], Loss: 0.09738414511084556\n",
      "Epoch [23/40], Step [110/235], Loss: 0.10710925981402397\n",
      "Epoch [23/40], Step [120/235], Loss: 0.10426508188247681\n",
      "Epoch [23/40], Step [130/235], Loss: 0.09034451581537724\n",
      "Epoch [23/40], Step [140/235], Loss: 0.10153299942612648\n",
      "Epoch [23/40], Step [150/235], Loss: 0.0935732837766409\n",
      "Epoch [23/40], Step [160/235], Loss: 0.0921593651175499\n",
      "Epoch [23/40], Step [170/235], Loss: 0.08243675343692303\n",
      "Epoch [23/40], Step [180/235], Loss: 0.1081103079020977\n",
      "Epoch [23/40], Step [190/235], Loss: 0.10758669152855874\n",
      "Epoch [23/40], Step [200/235], Loss: 0.10549938529729844\n",
      "Epoch [23/40], Step [210/235], Loss: 0.09261733293533325\n",
      "Epoch [23/40], Step [220/235], Loss: 0.09499588087201119\n",
      "Epoch [23/40], Step [230/235], Loss: 0.11574097834527493\n",
      "Epoch [23/40], Train Loss: 0.100832361474\n",
      "Epoch [23/40], Validation Loss: 0.103465010302\n",
      "Epoch [24/40], Step [10/235], Loss: 0.09257004074752331\n",
      "Epoch [24/40], Step [20/235], Loss: 0.10865488499403\n",
      "Epoch [24/40], Step [30/235], Loss: 0.10588533021509647\n",
      "Epoch [24/40], Step [40/235], Loss: 0.11237040236592293\n",
      "Epoch [24/40], Step [50/235], Loss: 0.12295285165309906\n",
      "Epoch [24/40], Step [60/235], Loss: 0.0941313948482275\n",
      "Epoch [24/40], Step [70/235], Loss: 0.12017568573355675\n",
      "Epoch [24/40], Step [80/235], Loss: 0.11370813995599746\n",
      "Epoch [24/40], Step [90/235], Loss: 0.10349424332380294\n",
      "Epoch [24/40], Step [100/235], Loss: 0.10396718829870225\n",
      "Epoch [24/40], Step [110/235], Loss: 0.09871151819825172\n",
      "Epoch [24/40], Step [120/235], Loss: 0.11335333213210105\n",
      "Epoch [24/40], Step [130/235], Loss: 0.091382946819067\n",
      "Epoch [24/40], Step [140/235], Loss: 0.07983252555131912\n",
      "Epoch [24/40], Step [150/235], Loss: 0.09690876379609108\n",
      "Epoch [24/40], Step [160/235], Loss: 0.09475096166133881\n",
      "Epoch [24/40], Step [170/235], Loss: 0.0986683875322342\n",
      "Epoch [24/40], Step [180/235], Loss: 0.1158368855714798\n",
      "Epoch [24/40], Step [190/235], Loss: 0.08362519592046738\n",
      "Epoch [24/40], Step [200/235], Loss: 0.08814054355025291\n",
      "Epoch [24/40], Step [210/235], Loss: 0.09280727058649063\n",
      "Epoch [24/40], Step [220/235], Loss: 0.0874612495303154\n",
      "Epoch [24/40], Step [230/235], Loss: 0.09588347300887108\n",
      "Epoch [24/40], Train Loss: 0.100776853840\n",
      "Epoch [24/40], Validation Loss: 0.103348972813\n",
      "Epoch [25/40], Step [10/235], Loss: 0.09174857586622238\n",
      "Epoch [25/40], Step [20/235], Loss: 0.10406229421496391\n",
      "Epoch [25/40], Step [30/235], Loss: 0.09165369644761086\n",
      "Epoch [25/40], Step [40/235], Loss: 0.0844031661748886\n",
      "Epoch [25/40], Step [50/235], Loss: 0.09859717786312103\n",
      "Epoch [25/40], Step [60/235], Loss: 0.11552561894059181\n",
      "Epoch [25/40], Step [70/235], Loss: 0.11435558125376702\n",
      "Epoch [25/40], Step [80/235], Loss: 0.10373472832143307\n",
      "Epoch [25/40], Step [90/235], Loss: 0.10429041609168052\n",
      "Epoch [25/40], Step [100/235], Loss: 0.11856684610247611\n",
      "Epoch [25/40], Step [110/235], Loss: 0.10218560397624969\n",
      "Epoch [25/40], Step [120/235], Loss: 0.1089419286698103\n",
      "Epoch [25/40], Step [130/235], Loss: 0.07966225184500217\n",
      "Epoch [25/40], Step [140/235], Loss: 0.11877246722579002\n",
      "Epoch [25/40], Step [150/235], Loss: 0.08480481021106243\n",
      "Epoch [25/40], Step [160/235], Loss: 0.12840096801519393\n",
      "Epoch [25/40], Step [170/235], Loss: 0.08288136124610901\n",
      "Epoch [25/40], Step [180/235], Loss: 0.10365695059299469\n",
      "Epoch [25/40], Step [190/235], Loss: 0.10506992898881436\n",
      "Epoch [25/40], Step [200/235], Loss: 0.09189914166927338\n",
      "Epoch [25/40], Step [210/235], Loss: 0.08497752994298935\n",
      "Epoch [25/40], Step [220/235], Loss: 0.08476846888661385\n",
      "Epoch [25/40], Step [230/235], Loss: 0.10828023999929429\n",
      "Epoch [25/40], Train Loss: 0.100812267132\n",
      "Epoch [25/40], Validation Loss: 0.103392319722\n",
      "Epoch [26/40], Step [10/235], Loss: 0.0985945075750351\n",
      "Epoch [26/40], Step [20/235], Loss: 0.08296913392841816\n",
      "Epoch [26/40], Step [30/235], Loss: 0.0983187235891819\n",
      "Epoch [26/40], Step [40/235], Loss: 0.09281849339604378\n",
      "Epoch [26/40], Step [50/235], Loss: 0.10475512444972992\n",
      "Epoch [26/40], Step [60/235], Loss: 0.10639001727104187\n",
      "Epoch [26/40], Step [70/235], Loss: 0.10611546337604523\n",
      "Epoch [26/40], Step [80/235], Loss: 0.09989531673491\n",
      "Epoch [26/40], Step [90/235], Loss: 0.10168464854359627\n",
      "Epoch [26/40], Step [100/235], Loss: 0.1093797042965889\n",
      "Epoch [26/40], Step [110/235], Loss: 0.1123152568936348\n",
      "Epoch [26/40], Step [120/235], Loss: 0.09908837154507637\n",
      "Epoch [26/40], Step [130/235], Loss: 0.11016628593206405\n",
      "Epoch [26/40], Step [140/235], Loss: 0.11264358162879944\n",
      "Epoch [26/40], Step [150/235], Loss: 0.10247707664966584\n",
      "Epoch [26/40], Step [160/235], Loss: 0.10845755487680435\n",
      "Epoch [26/40], Step [170/235], Loss: 0.0966023825109005\n",
      "Epoch [26/40], Step [180/235], Loss: 0.09900335147976876\n",
      "Epoch [26/40], Step [190/235], Loss: 0.10252451151609421\n",
      "Epoch [26/40], Step [200/235], Loss: 0.11703741252422332\n",
      "Epoch [26/40], Step [210/235], Loss: 0.08988704383373261\n",
      "Epoch [26/40], Step [220/235], Loss: 0.09036865010857582\n",
      "Epoch [26/40], Step [230/235], Loss: 0.09056737199425698\n",
      "Epoch [26/40], Train Loss: 0.101039554179\n",
      "Epoch [26/40], Validation Loss: 0.103587495763\n",
      "Epoch [27/40], Step [10/235], Loss: 0.08668040782213211\n",
      "Epoch [27/40], Step [20/235], Loss: 0.10302468612790108\n",
      "Epoch [27/40], Step [30/235], Loss: 0.09300488606095314\n",
      "Epoch [27/40], Step [40/235], Loss: 0.09799786135554314\n",
      "Epoch [27/40], Step [50/235], Loss: 0.10278807953000069\n",
      "Epoch [27/40], Step [60/235], Loss: 0.09155633188784122\n",
      "Epoch [27/40], Step [70/235], Loss: 0.10978217720985413\n",
      "Epoch [27/40], Step [80/235], Loss: 0.09833292998373508\n",
      "Epoch [27/40], Step [90/235], Loss: 0.10661814585328103\n",
      "Epoch [27/40], Step [100/235], Loss: 0.10365437418222427\n",
      "Epoch [27/40], Step [110/235], Loss: 0.10731316804885864\n",
      "Epoch [27/40], Step [120/235], Loss: 0.07757585942745208\n",
      "Epoch [27/40], Step [130/235], Loss: 0.10721146613359452\n",
      "Epoch [27/40], Step [140/235], Loss: 0.09213538467884064\n",
      "Epoch [27/40], Step [150/235], Loss: 0.11082526668906212\n",
      "Epoch [27/40], Step [160/235], Loss: 0.11620915457606315\n",
      "Epoch [27/40], Step [170/235], Loss: 0.10571498796343803\n",
      "Epoch [27/40], Step [180/235], Loss: 0.10194838270545006\n",
      "Epoch [27/40], Step [190/235], Loss: 0.10606026984751224\n",
      "Epoch [27/40], Step [200/235], Loss: 0.11862914860248566\n",
      "Epoch [27/40], Step [210/235], Loss: 0.08587901778519154\n",
      "Epoch [27/40], Step [220/235], Loss: 0.0774090189486742\n",
      "Epoch [27/40], Step [230/235], Loss: 0.10370172522962093\n",
      "Epoch [27/40], Train Loss: 0.101615364120\n",
      "Epoch [27/40], Validation Loss: 0.103320995882\n",
      "Epoch [28/40], Step [10/235], Loss: 0.1082902081310749\n",
      "Epoch [28/40], Step [20/235], Loss: 0.1056175947189331\n",
      "Epoch [28/40], Step [30/235], Loss: 0.10199691504240035\n",
      "Epoch [28/40], Step [40/235], Loss: 0.11122617498040199\n",
      "Epoch [28/40], Step [50/235], Loss: 0.09172323197126389\n",
      "Epoch [28/40], Step [60/235], Loss: 0.10689678639173508\n",
      "Epoch [28/40], Step [70/235], Loss: 0.1183943897485733\n",
      "Epoch [28/40], Step [80/235], Loss: 0.08854569494724274\n",
      "Epoch [28/40], Step [90/235], Loss: 0.11295752190053462\n",
      "Epoch [28/40], Step [100/235], Loss: 0.10028767362236976\n",
      "Epoch [28/40], Step [110/235], Loss: 0.08026439137756824\n",
      "Epoch [28/40], Step [120/235], Loss: 0.09784235954284667\n",
      "Epoch [28/40], Step [130/235], Loss: 0.09849550053477288\n",
      "Epoch [28/40], Step [140/235], Loss: 0.10733219236135483\n",
      "Epoch [28/40], Step [150/235], Loss: 0.10075187161564828\n",
      "Epoch [28/40], Step [160/235], Loss: 0.10569878742098808\n",
      "Epoch [28/40], Step [170/235], Loss: 0.09795046225190163\n",
      "Epoch [28/40], Step [180/235], Loss: 0.10655454322695732\n",
      "Epoch [28/40], Step [190/235], Loss: 0.09211398400366307\n",
      "Epoch [28/40], Step [200/235], Loss: 0.11539262682199478\n",
      "Epoch [28/40], Step [210/235], Loss: 0.08730111420154571\n",
      "Epoch [28/40], Step [220/235], Loss: 0.10344147309660912\n",
      "Epoch [28/40], Step [230/235], Loss: 0.08403830341994763\n",
      "Epoch [28/40], Train Loss: 0.101370163325\n",
      "Epoch [28/40], Validation Loss: 0.103444693326\n",
      "Epoch [29/40], Step [10/235], Loss: 0.11741389334201813\n",
      "Epoch [29/40], Step [20/235], Loss: 0.10111681818962097\n",
      "Epoch [29/40], Step [30/235], Loss: 0.09914666265249253\n",
      "Epoch [29/40], Step [40/235], Loss: 0.0794670395553112\n",
      "Epoch [29/40], Step [50/235], Loss: 0.09532256722450257\n",
      "Epoch [29/40], Step [60/235], Loss: 0.10445181168615818\n",
      "Epoch [29/40], Step [70/235], Loss: 0.08328846544027328\n",
      "Epoch [29/40], Step [80/235], Loss: 0.0988313365727663\n",
      "Epoch [29/40], Step [90/235], Loss: 0.10848888233304024\n",
      "Epoch [29/40], Step [100/235], Loss: 0.10825760066509246\n",
      "Epoch [29/40], Step [110/235], Loss: 0.10904592648148537\n",
      "Epoch [29/40], Step [120/235], Loss: 0.10951463356614113\n",
      "Epoch [29/40], Step [130/235], Loss: 0.09404446743428707\n",
      "Epoch [29/40], Step [140/235], Loss: 0.0896101262420416\n",
      "Epoch [29/40], Step [150/235], Loss: 0.1006597861647606\n",
      "Epoch [29/40], Step [160/235], Loss: 0.08421355262398719\n",
      "Epoch [29/40], Step [170/235], Loss: 0.10792731679975986\n",
      "Epoch [29/40], Step [180/235], Loss: 0.09815702587366104\n",
      "Epoch [29/40], Step [190/235], Loss: 0.09854641221463681\n",
      "Epoch [29/40], Step [200/235], Loss: 0.10781260654330253\n",
      "Epoch [29/40], Step [210/235], Loss: 0.11531385108828544\n",
      "Epoch [29/40], Step [220/235], Loss: 0.1076669603586197\n",
      "Epoch [29/40], Step [230/235], Loss: 0.10791089981794358\n",
      "Epoch [29/40], Train Loss: 0.100898109075\n",
      "Epoch [29/40], Validation Loss: 0.103307121449\n",
      "Epoch [30/40], Step [10/235], Loss: 0.09464146830141544\n",
      "Epoch [30/40], Step [20/235], Loss: 0.10920793786644936\n",
      "Epoch [30/40], Step [30/235], Loss: 0.10853144824504853\n",
      "Epoch [30/40], Step [40/235], Loss: 0.10609576776623726\n",
      "Epoch [30/40], Step [50/235], Loss: 0.11807282492518426\n",
      "Epoch [30/40], Step [60/235], Loss: 0.09004601538181305\n",
      "Epoch [30/40], Step [70/235], Loss: 0.12132035121321678\n",
      "Epoch [30/40], Step [80/235], Loss: 0.1032168723642826\n",
      "Epoch [30/40], Step [90/235], Loss: 0.08752132095396518\n",
      "Epoch [30/40], Step [100/235], Loss: 0.08952810764312744\n",
      "Epoch [30/40], Step [110/235], Loss: 0.11145944222807884\n",
      "Epoch [30/40], Step [120/235], Loss: 0.11129021868109704\n",
      "Epoch [30/40], Step [130/235], Loss: 0.11466138511896133\n",
      "Epoch [30/40], Step [140/235], Loss: 0.0883235439658165\n",
      "Epoch [30/40], Step [150/235], Loss: 0.0863055095076561\n",
      "Epoch [30/40], Step [160/235], Loss: 0.11009857505559921\n",
      "Epoch [30/40], Step [170/235], Loss: 0.0987424224615097\n",
      "Epoch [30/40], Step [180/235], Loss: 0.09208092913031578\n",
      "Epoch [30/40], Step [190/235], Loss: 0.09233230054378509\n",
      "Epoch [30/40], Step [200/235], Loss: 0.09221656620502472\n",
      "Epoch [30/40], Step [210/235], Loss: 0.09711498916149139\n",
      "Epoch [30/40], Step [220/235], Loss: 0.10694030448794364\n",
      "Epoch [30/40], Step [230/235], Loss: 0.08913796059787274\n",
      "Epoch [30/40], Train Loss: 0.100706825183\n",
      "Epoch [30/40], Validation Loss: 0.103932651738\n",
      "Epoch [31/40], Step [10/235], Loss: 0.09277067631483078\n",
      "Epoch [31/40], Step [20/235], Loss: 0.1044807929545641\n",
      "Epoch [31/40], Step [30/235], Loss: 0.09221099317073822\n",
      "Epoch [31/40], Step [40/235], Loss: 0.09681975021958351\n",
      "Epoch [31/40], Step [50/235], Loss: 0.10407766178250313\n",
      "Epoch [31/40], Step [60/235], Loss: 0.0947507344186306\n",
      "Epoch [31/40], Step [70/235], Loss: 0.09038303531706333\n",
      "Epoch [31/40], Step [80/235], Loss: 0.09187994673848152\n",
      "Epoch [31/40], Step [90/235], Loss: 0.09791215136647224\n",
      "Epoch [31/40], Step [100/235], Loss: 0.11731953844428063\n",
      "Epoch [31/40], Step [110/235], Loss: 0.10090272054076195\n",
      "Epoch [31/40], Step [120/235], Loss: 0.10745657309889793\n",
      "Epoch [31/40], Step [130/235], Loss: 0.10374914519488812\n",
      "Epoch [31/40], Step [140/235], Loss: 0.1056629866361618\n",
      "Epoch [31/40], Step [150/235], Loss: 0.089356629550457\n",
      "Epoch [31/40], Step [160/235], Loss: 0.11392609253525735\n",
      "Epoch [31/40], Step [170/235], Loss: 0.10175398588180543\n",
      "Epoch [31/40], Step [180/235], Loss: 0.0887833796441555\n",
      "Epoch [31/40], Step [190/235], Loss: 0.10360122695565224\n",
      "Epoch [31/40], Step [200/235], Loss: 0.10088341459631919\n",
      "Epoch [31/40], Step [210/235], Loss: 0.1212872512638569\n",
      "Epoch [31/40], Step [220/235], Loss: 0.08120502084493637\n",
      "Epoch [31/40], Step [230/235], Loss: 0.10924151986837387\n",
      "Epoch [31/40], Train Loss: 0.100563242611\n",
      "Epoch [31/40], Validation Loss: 0.103284605416\n",
      "Epoch [32/40], Step [10/235], Loss: 0.12828847542405128\n",
      "Epoch [32/40], Step [20/235], Loss: 0.10976760312914849\n",
      "Epoch [32/40], Step [30/235], Loss: 0.09899715408682823\n",
      "Epoch [32/40], Step [40/235], Loss: 0.09623981565237046\n",
      "Epoch [32/40], Step [50/235], Loss: 0.11043213121592999\n",
      "Epoch [32/40], Step [60/235], Loss: 0.10494540482759476\n",
      "Epoch [32/40], Step [70/235], Loss: 0.09703638181090354\n",
      "Epoch [32/40], Step [80/235], Loss: 0.1168260209262371\n",
      "Epoch [32/40], Step [90/235], Loss: 0.1106414906680584\n",
      "Epoch [32/40], Step [100/235], Loss: 0.10083809867501259\n",
      "Epoch [32/40], Step [110/235], Loss: 0.10946357920765877\n",
      "Epoch [32/40], Step [120/235], Loss: 0.09315110445022583\n",
      "Epoch [32/40], Step [130/235], Loss: 0.08032967410981655\n",
      "Epoch [32/40], Step [140/235], Loss: 0.0961696982383728\n",
      "Epoch [32/40], Step [150/235], Loss: 0.08825193047523498\n",
      "Epoch [32/40], Step [160/235], Loss: 0.08712150566279889\n",
      "Epoch [32/40], Step [170/235], Loss: 0.0902759276330471\n",
      "Epoch [32/40], Step [180/235], Loss: 0.09314994849264621\n",
      "Epoch [32/40], Step [190/235], Loss: 0.09608682803809643\n",
      "Epoch [32/40], Step [200/235], Loss: 0.11910629272460938\n",
      "Epoch [32/40], Step [210/235], Loss: 0.08797103501856327\n",
      "Epoch [32/40], Step [220/235], Loss: 0.09576289691030979\n",
      "Epoch [32/40], Step [230/235], Loss: 0.10361938178539276\n",
      "Epoch [32/40], Train Loss: 0.100807297468\n",
      "Epoch [32/40], Validation Loss: 0.104324350355\n",
      "Epoch [33/40], Step [10/235], Loss: 0.1153830848634243\n",
      "Epoch [33/40], Step [20/235], Loss: 0.0956112802028656\n",
      "Epoch [33/40], Step [30/235], Loss: 0.12518246844410896\n",
      "Epoch [33/40], Step [40/235], Loss: 0.10737782567739487\n",
      "Epoch [33/40], Step [50/235], Loss: 0.08832584992051125\n",
      "Epoch [33/40], Step [60/235], Loss: 0.09327319487929345\n",
      "Epoch [33/40], Step [70/235], Loss: 0.09289399608969688\n",
      "Epoch [33/40], Step [80/235], Loss: 0.09424306228756904\n",
      "Epoch [33/40], Step [90/235], Loss: 0.10199987478554248\n",
      "Epoch [33/40], Step [100/235], Loss: 0.09744171015918254\n",
      "Epoch [33/40], Step [110/235], Loss: 0.1184152290225029\n",
      "Epoch [33/40], Step [120/235], Loss: 0.1019309289753437\n",
      "Epoch [33/40], Step [130/235], Loss: 0.10417544469237328\n",
      "Epoch [33/40], Step [140/235], Loss: 0.08374004438519478\n",
      "Epoch [33/40], Step [150/235], Loss: 0.09488318711519242\n",
      "Epoch [33/40], Step [160/235], Loss: 0.08747535422444344\n",
      "Epoch [33/40], Step [170/235], Loss: 0.08632920011878013\n",
      "Epoch [33/40], Step [180/235], Loss: 0.1215233027935028\n",
      "Epoch [33/40], Step [190/235], Loss: 0.09466987401247025\n",
      "Epoch [33/40], Step [200/235], Loss: 0.10480916127562523\n",
      "Epoch [33/40], Step [210/235], Loss: 0.1041237086057663\n",
      "Epoch [33/40], Step [220/235], Loss: 0.09984039030969143\n",
      "Epoch [33/40], Step [230/235], Loss: 0.11041003689169884\n",
      "Epoch [33/40], Train Loss: 0.100533661357\n",
      "Epoch [33/40], Validation Loss: 0.103347018235\n",
      "Epoch [34/40], Step [10/235], Loss: 0.09705060943961144\n",
      "Epoch [34/40], Step [20/235], Loss: 0.090388448163867\n",
      "Epoch [34/40], Step [30/235], Loss: 0.10114019364118576\n",
      "Epoch [34/40], Step [40/235], Loss: 0.09259321913123131\n",
      "Epoch [34/40], Step [50/235], Loss: 0.08702663071453572\n",
      "Epoch [34/40], Step [60/235], Loss: 0.09721967875957489\n",
      "Epoch [34/40], Step [70/235], Loss: 0.09220623448491097\n",
      "Epoch [34/40], Step [80/235], Loss: 0.11051815785467625\n",
      "Epoch [34/40], Step [90/235], Loss: 0.10388561710715294\n",
      "Epoch [34/40], Step [100/235], Loss: 0.09602870270609856\n",
      "Epoch [34/40], Step [110/235], Loss: 0.11763330325484275\n",
      "Epoch [34/40], Step [120/235], Loss: 0.09371548146009445\n",
      "Epoch [34/40], Step [130/235], Loss: 0.11058303192257882\n",
      "Epoch [34/40], Step [140/235], Loss: 0.09757458902895451\n",
      "Epoch [34/40], Step [150/235], Loss: 0.10097687765955925\n",
      "Epoch [34/40], Step [160/235], Loss: 0.11847124844789506\n",
      "Epoch [34/40], Step [170/235], Loss: 0.09717185571789741\n",
      "Epoch [34/40], Step [180/235], Loss: 0.11469302773475647\n",
      "Epoch [34/40], Step [190/235], Loss: 0.09944699630141259\n",
      "Epoch [34/40], Step [200/235], Loss: 0.08127807565033436\n",
      "Epoch [34/40], Step [210/235], Loss: 0.104145547747612\n",
      "Epoch [34/40], Step [220/235], Loss: 0.12033610120415687\n",
      "Epoch [34/40], Step [230/235], Loss: 0.08643342442810535\n",
      "Epoch [34/40], Train Loss: 0.100938957327\n",
      "Epoch [34/40], Validation Loss: 0.103234102665\n",
      "Epoch [35/40], Step [10/235], Loss: 0.09417804703116417\n",
      "Epoch [35/40], Step [20/235], Loss: 0.09810146763920784\n",
      "Epoch [35/40], Step [30/235], Loss: 0.10886862091720104\n",
      "Epoch [35/40], Step [40/235], Loss: 0.10855403989553451\n",
      "Epoch [35/40], Step [50/235], Loss: 0.09297309070825577\n",
      "Epoch [35/40], Step [60/235], Loss: 0.08923649489879608\n",
      "Epoch [35/40], Step [70/235], Loss: 0.10029314011335373\n",
      "Epoch [35/40], Step [80/235], Loss: 0.10201515182852745\n",
      "Epoch [35/40], Step [90/235], Loss: 0.10977664738893508\n",
      "Epoch [35/40], Step [100/235], Loss: 0.09988735765218734\n",
      "Epoch [35/40], Step [110/235], Loss: 0.0902488611638546\n",
      "Epoch [35/40], Step [120/235], Loss: 0.12317271158099174\n",
      "Epoch [35/40], Step [130/235], Loss: 0.10730846822261811\n",
      "Epoch [35/40], Step [140/235], Loss: 0.11236509531736374\n",
      "Epoch [35/40], Step [150/235], Loss: 0.11283505260944367\n",
      "Epoch [35/40], Step [160/235], Loss: 0.10873516872525216\n",
      "Epoch [35/40], Step [170/235], Loss: 0.09617329761385918\n",
      "Epoch [35/40], Step [180/235], Loss: 0.08903998248279095\n",
      "Epoch [35/40], Step [190/235], Loss: 0.10281610749661922\n",
      "Epoch [35/40], Step [200/235], Loss: 0.08694390133023262\n",
      "Epoch [35/40], Step [210/235], Loss: 0.09341712407767773\n",
      "Epoch [35/40], Step [220/235], Loss: 0.08661497235298157\n",
      "Epoch [35/40], Step [230/235], Loss: 0.11066222861409188\n",
      "Epoch [35/40], Train Loss: 0.100800014588\n",
      "Epoch [35/40], Validation Loss: 0.103658905600\n",
      "Epoch [36/40], Step [10/235], Loss: 0.10512080863118171\n",
      "Epoch [36/40], Step [20/235], Loss: 0.09978068992495537\n",
      "Epoch [36/40], Step [30/235], Loss: 0.09462501592934132\n",
      "Epoch [36/40], Step [40/235], Loss: 0.09574229493737221\n",
      "Epoch [36/40], Step [50/235], Loss: 0.11447456032037735\n",
      "Epoch [36/40], Step [60/235], Loss: 0.09045913591980934\n",
      "Epoch [36/40], Step [70/235], Loss: 0.10295736268162728\n",
      "Epoch [36/40], Step [80/235], Loss: 0.10121617913246155\n",
      "Epoch [36/40], Step [90/235], Loss: 0.10461765266954899\n",
      "Epoch [36/40], Step [100/235], Loss: 0.10160768702626229\n",
      "Epoch [36/40], Step [110/235], Loss: 0.11115574762225151\n",
      "Epoch [36/40], Step [120/235], Loss: 0.11002295315265656\n",
      "Epoch [36/40], Step [130/235], Loss: 0.10014850609004497\n",
      "Epoch [36/40], Step [140/235], Loss: 0.10532814711332321\n",
      "Epoch [36/40], Step [150/235], Loss: 0.08539307191967964\n",
      "Epoch [36/40], Step [160/235], Loss: 0.10615652278065682\n",
      "Epoch [36/40], Step [170/235], Loss: 0.09641665816307068\n",
      "Epoch [36/40], Step [180/235], Loss: 0.09689282551407814\n",
      "Epoch [36/40], Step [190/235], Loss: 0.10525659061968326\n",
      "Epoch [36/40], Step [200/235], Loss: 0.0900593250989914\n",
      "Epoch [36/40], Step [210/235], Loss: 0.0899074736982584\n",
      "Epoch [36/40], Step [220/235], Loss: 0.09882327318191528\n",
      "Epoch [36/40], Step [230/235], Loss: 0.10016945526003837\n",
      "Epoch [36/40], Train Loss: 0.100962229025\n",
      "Epoch [36/40], Validation Loss: 0.103306616578\n",
      "Epoch [37/40], Step [10/235], Loss: 0.08700639866292477\n",
      "Epoch [37/40], Step [20/235], Loss: 0.09418629109859467\n",
      "Epoch [37/40], Step [30/235], Loss: 0.09791981056332588\n",
      "Epoch [37/40], Step [40/235], Loss: 0.09832717850804329\n",
      "Epoch [37/40], Step [50/235], Loss: 0.10086509510874749\n",
      "Epoch [37/40], Step [60/235], Loss: 0.10189976841211319\n",
      "Epoch [37/40], Step [70/235], Loss: 0.1067748248577118\n",
      "Epoch [37/40], Step [80/235], Loss: 0.1080443512648344\n",
      "Epoch [37/40], Step [90/235], Loss: 0.08725175634026527\n",
      "Epoch [37/40], Step [100/235], Loss: 0.11660711020231247\n",
      "Epoch [37/40], Step [110/235], Loss: 0.08539861664175988\n",
      "Epoch [37/40], Step [120/235], Loss: 0.09545522667467594\n",
      "Epoch [37/40], Step [130/235], Loss: 0.10728661119937896\n",
      "Epoch [37/40], Step [140/235], Loss: 0.10210240334272384\n",
      "Epoch [37/40], Step [150/235], Loss: 0.11803858131170272\n",
      "Epoch [37/40], Step [160/235], Loss: 0.11203718408942223\n",
      "Epoch [37/40], Step [170/235], Loss: 0.10506909489631652\n",
      "Epoch [37/40], Step [180/235], Loss: 0.10080773085355758\n",
      "Epoch [37/40], Step [190/235], Loss: 0.10416773147881031\n",
      "Epoch [37/40], Step [200/235], Loss: 0.0939171276986599\n",
      "Epoch [37/40], Step [210/235], Loss: 0.08497123792767525\n",
      "Epoch [37/40], Step [220/235], Loss: 0.11469935849308968\n",
      "Epoch [37/40], Step [230/235], Loss: 0.10523382797837258\n",
      "Epoch [37/40], Train Loss: 0.100902782238\n",
      "Epoch [37/40], Validation Loss: 0.103282164972\n",
      "Epoch [38/40], Step [10/235], Loss: 0.08625346049666405\n",
      "Epoch [38/40], Step [20/235], Loss: 0.10378713756799698\n",
      "Epoch [38/40], Step [30/235], Loss: 0.09354654178023339\n",
      "Epoch [38/40], Step [40/235], Loss: 0.0873958121985197\n",
      "Epoch [38/40], Step [50/235], Loss: 0.10156414955854416\n",
      "Epoch [38/40], Step [60/235], Loss: 0.11389294043183326\n",
      "Epoch [38/40], Step [70/235], Loss: 0.0901558306068182\n",
      "Epoch [38/40], Step [80/235], Loss: 0.10293511822819709\n",
      "Epoch [38/40], Step [90/235], Loss: 0.10781057700514793\n",
      "Epoch [38/40], Step [100/235], Loss: 0.10270623788237572\n",
      "Epoch [38/40], Step [110/235], Loss: 0.11291124820709228\n",
      "Epoch [38/40], Step [120/235], Loss: 0.10050384029746055\n",
      "Epoch [38/40], Step [130/235], Loss: 0.11966115608811378\n",
      "Epoch [38/40], Step [140/235], Loss: 0.10468897446990014\n",
      "Epoch [38/40], Step [150/235], Loss: 0.0914205215871334\n",
      "Epoch [38/40], Step [160/235], Loss: 0.09623597748577595\n",
      "Epoch [38/40], Step [170/235], Loss: 0.10535643063485622\n",
      "Epoch [38/40], Step [180/235], Loss: 0.10040860921144486\n",
      "Epoch [38/40], Step [190/235], Loss: 0.09940973967313767\n",
      "Epoch [38/40], Step [200/235], Loss: 0.08973734490573407\n",
      "Epoch [38/40], Step [210/235], Loss: 0.10414201691746712\n",
      "Epoch [38/40], Step [220/235], Loss: 0.0885685071349144\n",
      "Epoch [38/40], Step [230/235], Loss: 0.11266130283474922\n",
      "Epoch [38/40], Train Loss: 0.100437495731\n",
      "Epoch [38/40], Validation Loss: 0.103467725407\n",
      "Epoch [39/40], Step [10/235], Loss: 0.0982596717774868\n",
      "Epoch [39/40], Step [20/235], Loss: 0.11111037656664849\n",
      "Epoch [39/40], Step [30/235], Loss: 0.10693790800869465\n",
      "Epoch [39/40], Step [40/235], Loss: 0.09213169626891612\n",
      "Epoch [39/40], Step [50/235], Loss: 0.1127722404897213\n",
      "Epoch [39/40], Step [60/235], Loss: 0.08051265254616738\n",
      "Epoch [39/40], Step [70/235], Loss: 0.09868954420089722\n",
      "Epoch [39/40], Step [80/235], Loss: 0.10064913742244244\n",
      "Epoch [39/40], Step [90/235], Loss: 0.08388385511934757\n",
      "Epoch [39/40], Step [100/235], Loss: 0.1005026064813137\n",
      "Epoch [39/40], Step [110/235], Loss: 0.10105680972337723\n",
      "Epoch [39/40], Step [120/235], Loss: 0.12986151948571206\n",
      "Epoch [39/40], Step [130/235], Loss: 0.09912641160190105\n",
      "Epoch [39/40], Step [140/235], Loss: 0.09856767654418945\n",
      "Epoch [39/40], Step [150/235], Loss: 0.09286407753825188\n",
      "Epoch [39/40], Step [160/235], Loss: 0.0887189194560051\n",
      "Epoch [39/40], Step [170/235], Loss: 0.10074749626219273\n",
      "Epoch [39/40], Step [180/235], Loss: 0.10474600493907929\n",
      "Epoch [39/40], Step [190/235], Loss: 0.10156742073595523\n",
      "Epoch [39/40], Step [200/235], Loss: 0.11483411714434624\n",
      "Epoch [39/40], Step [210/235], Loss: 0.10531890578567982\n",
      "Epoch [39/40], Step [220/235], Loss: 0.08876629248261451\n",
      "Epoch [39/40], Step [230/235], Loss: 0.10390133708715439\n",
      "Epoch [39/40], Train Loss: 0.100880431970\n",
      "Epoch [39/40], Validation Loss: 0.103384712616\n",
      "Epoch [40/40], Step [10/235], Loss: 0.09437187388539314\n",
      "Epoch [40/40], Step [20/235], Loss: 0.11361296065151691\n",
      "Epoch [40/40], Step [30/235], Loss: 0.12022614777088166\n",
      "Epoch [40/40], Step [40/235], Loss: 0.08421700969338416\n",
      "Epoch [40/40], Step [50/235], Loss: 0.09903985038399696\n",
      "Epoch [40/40], Step [60/235], Loss: 0.10151672214269639\n",
      "Epoch [40/40], Step [70/235], Loss: 0.08775662034749984\n",
      "Epoch [40/40], Step [80/235], Loss: 0.0830904521048069\n",
      "Epoch [40/40], Step [90/235], Loss: 0.11012844666838646\n",
      "Epoch [40/40], Step [100/235], Loss: 0.09644907340407372\n",
      "Epoch [40/40], Step [110/235], Loss: 0.11727427393198013\n",
      "Epoch [40/40], Step [120/235], Loss: 0.09187787920236587\n",
      "Epoch [40/40], Step [130/235], Loss: 0.08862536661326885\n",
      "Epoch [40/40], Step [140/235], Loss: 0.11040125042200089\n",
      "Epoch [40/40], Step [150/235], Loss: 0.1124516062438488\n",
      "Epoch [40/40], Step [160/235], Loss: 0.09931368045508862\n",
      "Epoch [40/40], Step [170/235], Loss: 0.08226528726518154\n",
      "Epoch [40/40], Step [180/235], Loss: 0.10627429261803627\n",
      "Epoch [40/40], Step [190/235], Loss: 0.08724583573639393\n",
      "Epoch [40/40], Step [200/235], Loss: 0.11737918108701706\n",
      "Epoch [40/40], Step [210/235], Loss: 0.1072401762008667\n",
      "Epoch [40/40], Step [220/235], Loss: 0.10380203425884246\n",
      "Epoch [40/40], Step [230/235], Loss: 0.09679596498608589\n",
      "Epoch [40/40], Train Loss: 0.100483872741\n",
      "Epoch [40/40], Validation Loss: 0.103364288302\n"
     ]
    }
   ],
   "source": [
    "# batch_size = 16, lr = 0.001, epochs = 20 accuracy = 0.2638\n",
    "# 16, 0.001 ,30 accuracy = 0.2616\n",
    "# 32, 0,001, 30 accuracy = 0.2638\n",
    "# 64, 0.001, 30 0.2583\n",
    "# 64, 0.0005, 50 0.26\n",
    "# 32, 0.001 , 40 0.\n",
    "\n",
    "\n",
    "# 损失函数和优化器\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.001)\n",
    "log_train_loss_list = []\n",
    "log_val_loss_list = []\n",
    "# 训练模型\n",
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # 零梯度\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        train_loss += loss.item()\n",
    "        running_loss += loss.item()\n",
    "        # 反向传播和优化\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i + 1) % 10 == 0:  # 每 10 个批次打印一次\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {running_loss/10:}')\n",
    "            running_loss = 0\n",
    "\n",
    "    train_loss_avg = train_loss/len(train_loader)\n",
    "    #log_train_loss_list.append(train_loss_avg)\n",
    "    log_train_loss_list.append(np.log(train_loss_avg))\n",
    "            \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss_avg:.12f}')\n",
    "\n",
    "    # 计算在验证集上的 loss\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(val_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    val_loss_avg = val_loss / len(val_loader)\n",
    "    #log_val_loss_list.append(val_loss_avg)\n",
    "    log_val_loss_list.append(np.log(val_loss_avg))\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss_avg:.12f}')\n",
    "\n",
    "#储存模型\n",
    "torch.save(model.state_dict(), 'CNN.pth')\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 40\n",
      "[-2.2454860272951613, -2.2602724316403475, -2.2546157036956798, -2.2768988560252352, -2.2896145356920097, -2.2902730772506414, -2.291721667667327, -2.284324863488342, -2.291753240333205, -2.2883562368613877, -2.2915120543032996, -2.2863815469825677, -2.2902531334448386, -2.292933315802002, -2.2917370853659214, -2.293506853507988, -2.2903079207556973, -2.291969524999057, -2.290141098363013, -2.2921402232069052, -2.28941406029269, -2.2948851389667797, -2.2942959284970437, -2.294846574311822, -2.294495233015034, -2.2922432132743116, -2.286560533613495, -2.2889764784092943, -2.2936440923805708, -2.295541704161881, -2.296968469680882, -2.294544530453875, -2.297262668684244, -2.293239327752618, -2.2946167786184204, -2.2930088021297594, -2.293597777791534, -2.2982196779918485, -2.293819305312602, -2.297758034593953] [-2.176840684455793, -2.2404711810189397, -2.0444378600037436, -2.2645782373925494, -2.2643615134018544, -2.2589123976887, -2.253312151495441, -2.265621265061626, -2.266513791724655, -2.262867068600832, -2.266971447073275, -2.2675238576427676, -2.2679023176067186, -2.2657341290633677, -2.258517807592851, -2.26639266800803, -2.2609628071831898, -2.268254788855223, -2.256862979642989, -2.2483747472609417, -2.268544287456111, -2.2692076597091417, -2.2685217881528787, -2.269643931824433, -2.26922459701471, -2.2673386537103877, -2.2699146719855605, -2.2687181731006194, -2.2700489657448353, -2.2640121690484514, -2.2702669418839365, -2.2602504796551814, -2.269662844409308, -2.270756028338176, -2.266649523867596, -2.270053852850127, -2.270290570505172, -2.2684955467250707, -2.269298174876307, -2.269495750820572]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(len(log_train_loss_list), len(log_val_loss_list))\n",
    "print(log_train_loss_list, log_val_loss_list)\n",
    "#绘制loss曲线\n",
    "plt.plot(log_train_loss_list, label='train')\n",
    "\n",
    "plt.plot(log_val_loss_list, label='validation')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('log_Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_882038/239606473.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state_dict = torch.load('CNN.pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7354043845783032e-05\n",
      "9.395956445425498e-06\n",
      "1.601249960020295e-05\n",
      "2.4536764424069534e-06\n",
      "7.312500032490708e-06\n",
      "8.207352215162846e-06\n",
      "1.2761028710899413e-05\n",
      "2.0816175539807576e-05\n",
      "1.704411882358542e-05\n",
      "1.6055146976386534e-05\n",
      "3.928676476782433e-06\n",
      "3.3882351600981877e-06\n",
      "7.898161229459756e-05\n",
      "2.5243376566383292e-05\n",
      "1.1937499591309819e-05\n",
      "5.573529570914926e-06\n",
      "1.0418014677945482e-05\n",
      "1.4122425007600542e-05\n",
      "1.0139705448819979e-06\n",
      "0.00014821139761653205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/aiia/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/anaconda3/envs/aiia/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/anaconda3/envs/aiia/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/anaconda3/envs/aiia/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/anaconda3/envs/aiia/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/anaconda3/envs/aiia/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/anaconda3/envs/aiia/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/anaconda3/envs/aiia/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试 Step [1/20], MSE: 0.000000003286\n",
      "预测部分正确！预测级别为C2, 真实级别为C8\n",
      "预测值为0.000001991, 真实值为0.000007647\n",
      "预测部分正确！预测级别为C2, 真实级别为C8\n",
      "预测值为0.000001911, 真实值为0.000007647\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001854, 真实值为0.000001529\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000001888, 真实值为0.000001412\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000001809, 真实值为0.000001153\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000001832, 真实值为0.000001176\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001835, 真实值为0.000001765\n",
      "预测错误！预测级别为C2, 真实级别为X2\n",
      "预测值为0.000001955, 真实值为0.000200000\n",
      "预测错误！预测级别为C2, 真实级别为X2\n",
      "预测值为0.000002352, 真实值为0.000200000\n",
      "预测错误！预测级别为C2, 真实级别为M4\n",
      "预测值为0.000002352, 真实值为0.000037647\n",
      "预测错误！预测级别为C2, 真实级别为M4\n",
      "预测值为0.000001843, 真实值为0.000037647\n",
      "预测错误！预测级别为C2, 真实级别为X1\n",
      "预测值为0.000001876, 真实值为0.000102353\n",
      "预测错误！预测级别为C2, 真实级别为X1\n",
      "预测值为0.000001949, 真实值为0.000102353\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000001808, 真实值为0.000001118\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000001828, 真实值为0.000002941\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000001852, 真实值为0.000002941\n",
      "预测部分正确！预测级别为C2, 真实级别为C9\n",
      "预测值为0.000001834, 真实值为0.000009294\n",
      "预测部分正确！预测级别为C2, 真实级别为C7\n",
      "预测值为0.000001839, 真实值为0.000006824\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001940, 真实值为0.000002000\n",
      "预测错误！预测级别为C2, 真实级别为M2\n",
      "预测值为0.000001912, 真实值为0.000020000\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000001904, 真实值为0.000002941\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000001912, 真实值为0.000002941\n",
      "预测部分正确！预测级别为C2, 真实级别为C4\n",
      "预测值为0.000001913, 真实值为0.000003765\n",
      "预测部分正确！预测级别为C2, 真实级别为C8\n",
      "预测值为0.000001885, 真实值为0.000008235\n",
      "预测部分正确！预测级别为C2, 真实级别为C4\n",
      "预测值为0.000001959, 真实值为0.000003765\n",
      "预测部分正确！预测级别为C2, 真实级别为C9\n",
      "预测值为0.000001950, 真实值为0.000008706\n",
      "预测部分正确！预测级别为C2, 真实级别为C9\n",
      "预测值为0.000002352, 真实值为0.000008706\n",
      "预测部分正确！预测级别为C2, 真实级别为C5\n",
      "预测值为0.000001920, 真实值为0.000004824\n",
      "预测错误！预测级别为C2, 真实级别为M2\n",
      "预测值为0.000001972, 真实值为0.000015294\n",
      "预测部分正确！预测级别为C2, 真实级别为C5\n",
      "预测值为0.000001922, 真实值为0.000005176\n",
      "预测部分正确！预测级别为C2, 真实级别为C4\n",
      "预测值为0.000001922, 真实值为0.000003529\n",
      "预测错误！预测级别为C2, 真实级别为M6\n",
      "预测值为0.000002010, 真实值为0.000060000\n",
      "测试 Step [2/20], MSE: 0.000000000324\n",
      "预测部分正确！预测级别为C2, 真实级别为C7\n",
      "预测值为0.000001918, 真实值为0.000007176\n",
      "预测错误！预测级别为C2, 真实级别为M6\n",
      "预测值为0.000001990, 真实值为0.000055294\n",
      "预测错误！预测级别为C2, 真实级别为M7\n",
      "预测值为0.000001916, 真实值为0.000067059\n",
      "预测部分正确！预测级别为C2, 真实级别为C4\n",
      "预测值为0.000002042, 真实值为0.000004235\n",
      "预测部分正确！预测级别为C2, 真实级别为C5\n",
      "预测值为0.000001885, 真实值为0.000004824\n",
      "预测部分正确！预测级别为C2, 真实级别为C5\n",
      "预测值为0.000002352, 真实值为0.000004824\n",
      "预测部分正确！预测级别为C2, 真实级别为C9\n",
      "预测值为0.000001950, 真实值为0.000009294\n",
      "预测错误！预测级别为C2, 真实级别为M2\n",
      "预测值为0.000002009, 真实值为0.000016471\n",
      "预测错误！预测级别为C2, 真实级别为M2\n",
      "预测值为0.000002352, 真实值为0.000016471\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000002012, 真实值为0.000001094\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000002021, 真实值为0.000001094\n",
      "预测错误！预测级别为C2, 真实级别为B8\n",
      "预测值为0.000001988, 真实值为0.000000800\n",
      "预测部分正确！预测级别为C2, 真实级别为C5\n",
      "预测值为0.000001997, 真实值为0.000004588\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000001936, 真实值为0.000001176\n",
      "预测错误！预测级别为C2, 真实级别为M4\n",
      "预测值为0.000001965, 真实值为0.000038824\n",
      "预测错误！预测级别为C2, 真实级别为M4\n",
      "预测值为0.000001873, 真实值为0.000038824\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001931, 真实值为0.000001882\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001904, 真实值为0.000001765\n",
      "预测部分正确！预测级别为C2, 真实级别为C4\n",
      "预测值为0.000001921, 真实值为0.000004118\n",
      "预测部分正确！预测级别为C2, 真实级别为C4\n",
      "预测值为0.000001939, 真实值为0.000004118\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000001863, 真实值为0.000001294\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000002352, 真实值为0.000001294\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000001990, 真实值为0.000001000\n",
      "预测错误！预测级别为C2, 真实级别为B8\n",
      "预测值为0.000002023, 真实值为0.000000824\n",
      "预测错误！预测级别为C2, 真实级别为B8\n",
      "预测值为0.000001977, 真实值为0.000000824\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001988, 真实值为0.000001647\n",
      "预测错误！预测级别为C2, 真实级别为B7\n",
      "预测值为0.000002002, 真实值为0.000000671\n",
      "预测错误！预测级别为C2, 真实级别为B7\n",
      "预测值为0.000002028, 真实值为0.000000671\n",
      "预测部分正确！预测级别为C2, 真实级别为C5\n",
      "预测值为0.000002007, 真实值为0.000004941\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000002005, 真实值为0.000001294\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000001988, 真实值为0.000001294\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000001967, 真实值为0.000000988\n",
      "测试 Step [3/20], MSE: 0.000000002769\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000001987, 真实值为0.000000988\n",
      "预测错误！预测级别为C2, 真实级别为B5\n",
      "预测值为0.000002017, 真实值为0.000000506\n",
      "预测错误！预测级别为C2, 真实级别为B8\n",
      "预测值为0.000001918, 真实值为0.000000847\n",
      "预测错误！预测级别为C2, 真实级别为X2\n",
      "预测值为0.000002036, 真实值为0.000211765\n",
      "预测错误！预测级别为C2, 真实级别为X2\n",
      "预测值为0.000002352, 真实值为0.000211765\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000001983, 真实值为0.000001176\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000001958, 真实值为0.000001294\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000001971, 真实值为0.000003412\n",
      "预测错误！预测级别为C2, 真实级别为B8\n",
      "预测值为0.000002050, 真实值为0.000000812\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000001929, 真实值为0.000001141\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000002352, 真实值为0.000001141\n",
      "预测错误！预测级别为C2, 真实级别为B7\n",
      "预测值为0.000002033, 真实值为0.000000682\n",
      "预测错误！预测级别为C2, 真实级别为B7\n",
      "预测值为0.000002022, 真实值为0.000000682\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001984, 真实值为0.000002000\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001997, 真实值为0.000001529\n",
      "预测错误！预测级别为C2, 真实级别为B5\n",
      "预测值为0.000002043, 真实值为0.000000459\n",
      "预测错误！预测级别为C2, 真实级别为B4\n",
      "预测值为0.000001979, 真实值为0.000000376\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000002001, 真实值为0.000000976\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000002003, 真实值为0.000000976\n",
      "预测错误！预测级别为C2, 真实级别为M3\n",
      "预测值为0.000001932, 真实值为0.000025882\n",
      "预测部分正确！预测级别为C2, 真实级别为C4\n",
      "预测值为0.000001921, 真实值为0.000004353\n",
      "预测部分正确！预测级别为C2, 真实级别为C7\n",
      "预测值为0.000001875, 真实值为0.000006941\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001929, 真实值为0.000002000\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001895, 真实值为0.000002235\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001973, 真实值为0.000001647\n",
      "预测错误！预测级别为C2, 真实级别为M1\n",
      "预测值为0.000001885, 真实值为0.000011765\n",
      "预测部分正确！预测级别为C2, 真实级别为C6\n",
      "预测值为0.000001976, 真实值为0.000006353\n",
      "预测部分正确！预测级别为C2, 真实级别为C6\n",
      "预测值为0.000002352, 真实值为0.000006353\n",
      "预测错误！预测级别为C2, 真实级别为B3\n",
      "预测值为0.000001995, 真实值为0.000000329\n",
      "预测错误！预测级别为C2, 真实级别为B7\n",
      "预测值为0.000001988, 真实值为0.000000671\n",
      "预测错误！预测级别为C2, 真实级别为B7\n",
      "预测值为0.000002000, 真实值为0.000000671\n",
      "预测错误！预测级别为C2, 真实级别为B7\n",
      "预测值为0.000001986, 真实值为0.000000671\n",
      "测试 Step [4/20], MSE: 0.000000000011\n",
      "预测错误！预测级别为C2, 真实级别为B7\n",
      "预测值为0.000002352, 真实值为0.000000671\n",
      "预测错误！预测级别为C2, 真实级别为B4\n",
      "预测值为0.000002040, 真实值为0.000000400\n",
      "预测错误！预测级别为C2, 真实级别为B4\n",
      "预测值为0.000001969, 真实值为0.000000400\n",
      "预测错误！预测级别为C2, 真实级别为B6\n",
      "预测值为0.000002012, 真实值为0.000000553\n",
      "预测错误！预测级别为C2, 真实级别为B6\n",
      "预测值为0.000001998, 真实值为0.000000553\n",
      "预测错误！预测级别为C2, 真实级别为B8\n",
      "预测值为0.000001978, 真实值为0.000000753\n",
      "预测错误！预测级别为C2, 真实级别为B7\n",
      "预测值为0.000001996, 真实值为0.000000706\n",
      "预测错误！预测级别为C2, 真实级别为B7\n",
      "预测值为0.000001992, 真实值为0.000000671\n",
      "预测错误！预测级别为C2, 真实级别为B5\n",
      "预测值为0.000001973, 真实值为0.000000471\n",
      "预测错误！预测级别为C2, 真实级别为B5\n",
      "预测值为0.000002352, 真实值为0.000000471\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001919, 真实值为0.000002353\n",
      "预测错误！预测级别为C2, 真实级别为B8\n",
      "预测值为0.000001873, 真实值为0.000000765\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000001934, 真实值为0.000001082\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000001996, 真实值为0.000001094\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001997, 真实值为0.000002000\n",
      "预测错误！预测级别为C2, 真实级别为M1\n",
      "预测值为0.000002020, 真实值为0.000011294\n",
      "预测错误！预测级别为C2, 真实级别为M1\n",
      "预测值为0.000001920, 真实值为0.000011294\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000002003, 真实值为0.000002000\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000002000, 真实值为0.000002706\n",
      "预测部分正确！预测级别为C2, 真实级别为C4\n",
      "预测值为0.000001996, 真实值为0.000004235\n",
      "预测部分正确！预测级别为C2, 真实级别为C7\n",
      "预测值为0.000001995, 真实值为0.000006941\n",
      "预测部分正确！预测级别为C2, 真实级别为C6\n",
      "预测值为0.000001970, 真实值为0.000006471\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000002030, 真实值为0.000001176\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000001929, 真实值为0.000001106\n",
      "预测错误！预测级别为C2, 真实级别为B5\n",
      "预测值为0.000001913, 真实值为0.000000541\n",
      "预测错误！预测级别为C2, 真实级别为B6\n",
      "预测值为0.000001925, 真实值为0.000000635\n",
      "预测错误！预测级别为C2, 真实级别为B5\n",
      "预测值为0.000001937, 真实值为0.000000506\n",
      "预测错误！预测级别为C2, 真实级别为B7\n",
      "预测值为0.000001959, 真实值为0.000000671\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000001938, 真实值为0.000001294\n",
      "预测错误！预测级别为C2, 真实级别为M1\n",
      "预测值为0.000001883, 真实值为0.000011176\n",
      "预测错误！预测级别为C2, 真实级别为B8\n",
      "预测值为0.000001991, 真实值为0.000000824\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000002021, 真实值为0.000002706\n",
      "测试 Step [5/20], MSE: 0.000000000303\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001952, 真实值为0.000002118\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000001940, 真实值为0.000001176\n",
      "预测错误！预测级别为C2, 真实级别为B9\n",
      "预测值为0.000001938, 真实值为0.000000894\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000001970, 真实值为0.000001035\n",
      "预测部分正确！预测级别为C2, 真实级别为C4\n",
      "预测值为0.000001965, 真实值为0.000003529\n",
      "预测错误！预测级别为C2, 真实级别为M2\n",
      "预测值为0.000002007, 真实值为0.000015294\n",
      "预测错误！预测级别为C2, 真实级别为M2\n",
      "预测值为0.000002035, 真实值为0.000015294\n",
      "预测错误！预测级别为C2, 真实级别为M7\n",
      "预测值为0.000001999, 真实值为0.000069412\n",
      "预测错误！预测级别为C2, 真实级别为M7\n",
      "预测值为0.000002013, 真实值为0.000069412\n",
      "预测错误！预测级别为C2, 真实级别为B8\n",
      "预测值为0.000001942, 真实值为0.000000812\n",
      "预测错误！预测级别为C2, 真实级别为B8\n",
      "预测值为0.000002016, 真实值为0.000000812\n",
      "预测错误！预测级别为C2, 真实级别为B8\n",
      "预测值为0.000001929, 真实值为0.000000847\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001935, 真实值为0.000001765\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000002018, 真实值为0.000001765\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000001982, 真实值为0.000001176\n",
      "预测错误！预测级别为C2, 真实级别为B8\n",
      "预测值为0.000001950, 真实值为0.000000800\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000001985, 真实值为0.000001176\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000002021, 真实值为0.000001176\n",
      "预测错误！预测级别为C2, 真实级别为B9\n",
      "预测值为0.000001980, 真实值为0.000000918\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000001984, 真实值为0.000001082\n",
      "预测错误！预测级别为C2, 真实级别为B9\n",
      "预测值为0.000001977, 真实值为0.000000929\n",
      "预测错误！预测级别为C2, 真实级别为B5\n",
      "预测值为0.000002017, 真实值为0.000000482\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001956, 真实值为0.000001529\n",
      "预测错误！预测级别为C2, 真实级别为B8\n",
      "预测值为0.000001995, 真实值为0.000000788\n",
      "预测错误！预测级别为C2, 真实级别为M1\n",
      "预测值为0.000001970, 真实值为0.000011647\n",
      "预测错误！预测级别为C2, 真实级别为M1\n",
      "预测值为0.000002352, 真实值为0.000011647\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000001981, 真实值为0.000001412\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000001999, 真实值为0.000001071\n",
      "预测部分正确！预测级别为C2, 真实级别为C9\n",
      "预测值为0.000001925, 真实值为0.000008588\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000002001, 真实值为0.000001765\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001959, 真实值为0.000001765\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001987, 真实值为0.000001882\n",
      "测试 Step [6/20], MSE: 0.000000000365\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001995, 真实值为0.000001882\n",
      "预测错误！预测级别为C2, 真实级别为M3\n",
      "预测值为0.000001971, 真实值为0.000034118\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001926, 真实值为0.000002118\n",
      "预测错误！预测级别为C2, 真实级别为M1\n",
      "预测值为0.000001924, 真实值为0.000011412\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001889, 真实值为0.000002353\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000002013, 真实值为0.000002118\n",
      "预测部分正确！预测级别为C2, 真实级别为C5\n",
      "预测值为0.000001906, 真实值为0.000004824\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000001983, 真实值为0.000003059\n",
      "预测部分正确！预测级别为C2, 真实级别为C4\n",
      "预测值为0.000001974, 真实值为0.000004118\n",
      "预测部分正确！预测级别为C2, 真实级别为C4\n",
      "预测值为0.000001969, 真实值为0.000004118\n",
      "预测错误！预测级别为C2, 真实级别为B4\n",
      "预测值为0.000001984, 真实值为0.000000435\n",
      "预测错误！预测级别为C2, 真实级别为B5\n",
      "预测值为0.000002001, 真实值为0.000000482\n",
      "预测错误！预测级别为C2, 真实级别为B5\n",
      "预测值为0.000002352, 真实值为0.000000482\n",
      "预测错误！预测级别为C2, 真实级别为B6\n",
      "预测值为0.000002352, 真实值为0.000000565\n",
      "预测错误！预测级别为C2, 真实级别为B4\n",
      "预测值为0.000001994, 真实值为0.000000424\n",
      "预测错误！预测级别为C2, 真实级别为B4\n",
      "预测值为0.000002352, 真实值为0.000000435\n",
      "预测错误！预测级别为C2, 真实级别为B5\n",
      "预测值为0.000001978, 真实值为0.000000494\n",
      "预测错误！预测级别为C2, 真实级别为B6\n",
      "预测值为0.000002009, 真实值为0.000000647\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000001990, 真实值为0.000002941\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000001908, 真实值为0.000001118\n",
      "预测部分正确！预测级别为C2, 真实级别为C7\n",
      "预测值为0.000002015, 真实值为0.000007294\n",
      "预测错误！预测级别为C2, 真实级别为B9\n",
      "预测值为0.000001942, 真实值为0.000000894\n",
      "预测错误！预测级别为C2, 真实级别为B9\n",
      "预测值为0.000002008, 真实值为0.000000894\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000001985, 真实值为0.000002706\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001954, 真实值为0.000002471\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000002352, 真实值为0.000002471\n",
      "预测部分正确！预测级别为C2, 真实级别为C4\n",
      "预测值为0.000001885, 真实值为0.000003765\n",
      "预测部分正确！预测级别为C2, 真实级别为C7\n",
      "预测值为0.000001884, 真实值为0.000007294\n",
      "预测部分正确！预测级别为C2, 真实级别为C7\n",
      "预测值为0.000002352, 真实值为0.000007294\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000001892, 真实值为0.000001176\n",
      "预测错误！预测级别为C2, 真实级别为M7\n",
      "预测值为0.000001827, 真实值为0.000074118\n",
      "预测错误！预测级别为C2, 真实级别为M7\n",
      "预测值为0.000002054, 真实值为0.000074118\n",
      "测试 Step [7/20], MSE: 0.000000000364\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001946, 真实值为0.000001765\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000001886, 真实值为0.000001412\n",
      "预测错误！预测级别为C2, 真实级别为M6\n",
      "预测值为0.000001908, 真实值为0.000058824\n",
      "预测错误！预测级别为C2, 真实级别为M6\n",
      "预测值为0.000002352, 真实值为0.000058824\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000001923, 真实值为0.000003412\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000001912, 真实值为0.000002588\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001955, 真实值为0.000001765\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000001895, 真实值为0.000002706\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000002352, 真实值为0.000002706\n",
      "预测部分正确！预测级别为C2, 真实级别为C4\n",
      "预测值为0.000001894, 真实值为0.000004000\n",
      "预测部分正确！预测级别为C2, 真实级别为C4\n",
      "预测值为0.000002352, 真实值为0.000004000\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001980, 真实值为0.000002235\n",
      "预测部分正确！预测级别为C2, 真实级别为C4\n",
      "预测值为0.000001940, 真实值为0.000004235\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000002352, 真实值为0.000001647\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000001912, 真实值为0.000003059\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000002025, 真实值为0.000002588\n",
      "预测部分正确！预测级别为C2, 真实级别为C6\n",
      "预测值为0.000001893, 真实值为0.000005647\n",
      "预测部分正确！预测级别为C2, 真实级别为C6\n",
      "预测值为0.000001817, 真实值为0.000005647\n",
      "预测错误！预测级别为C2, 真实级别为M1\n",
      "预测值为0.000001777, 真实值为0.000010118\n",
      "预测错误！预测级别为C2, 真实级别为M2\n",
      "预测值为0.000001787, 真实值为0.000023529\n",
      "预测错误！预测级别为C2, 真实级别为M2\n",
      "预测值为0.000001815, 真实值为0.000023529\n",
      "预测错误！预测级别为C2, 真实级别为M2\n",
      "预测值为0.000001779, 真实值为0.000017647\n",
      "预测错误！预测级别为C2, 真实级别为M4\n",
      "预测值为0.000001915, 真实值为0.000035294\n",
      "预测错误！预测级别为C2, 真实级别为M4\n",
      "预测值为0.000001794, 真实值为0.000035294\n",
      "预测部分正确！预测级别为C2, 真实级别为C6\n",
      "预测值为0.000001948, 真实值为0.000005647\n",
      "预测部分正确！预测级别为C2, 真实级别为C6\n",
      "预测值为0.000001768, 真实值为0.000005765\n",
      "预测错误！预测级别为C2, 真实级别为M4\n",
      "预测值为0.000001778, 真实值为0.000036471\n",
      "预测错误！预测级别为C2, 真实级别为M1\n",
      "预测值为0.000001869, 真实值为0.000010118\n",
      "预测错误！预测级别为C2, 真实级别为M2\n",
      "预测值为0.000001730, 真实值为0.000015294\n",
      "预测错误！预测级别为C2, 真实级别为M2\n",
      "预测值为0.000002352, 真实值为0.000015294\n",
      "预测部分正确！预测级别为C2, 真实级别为C4\n",
      "预测值为0.000001871, 真实值为0.000003765\n",
      "预测部分正确！预测级别为C2, 真实级别为C4\n",
      "预测值为0.000001879, 真实值为0.000003529\n",
      "测试 Step [8/20], MSE: 0.000000000913\n",
      "预测错误！预测级别为C2, 真实级别为M3\n",
      "预测值为0.000001809, 真实值为0.000025882\n",
      "预测部分正确！预测级别为C2, 真实级别为C8\n",
      "预测值为0.000001912, 真实值为0.000008235\n",
      "预测错误！预测级别为C2, 真实级别为M5\n",
      "预测值为0.000001886, 真实值为0.000051765\n",
      "预测错误！预测级别为C2, 真实级别为M5\n",
      "预测值为0.000001823, 真实值为0.000051765\n",
      "预测部分正确！预测级别为C2, 真实级别为C4\n",
      "预测值为0.000001774, 真实值为0.000003765\n",
      "预测部分正确！预测级别为C2, 真实级别为C4\n",
      "预测值为0.000001919, 真实值为0.000004000\n",
      "预测错误！预测级别为C2, 真实级别为M2\n",
      "预测值为0.000001898, 真实值为0.000024706\n",
      "预测部分正确！预测级别为C2, 真实级别为C4\n",
      "预测值为0.000001778, 真实值为0.000003529\n",
      "预测错误！预测级别为C2, 真实级别为M2\n",
      "预测值为0.000001795, 真实值为0.000015294\n",
      "预测错误！预测级别为C2, 真实级别为M6\n",
      "预测值为0.000001935, 真实值为0.000057647\n",
      "预测部分正确！预测级别为C2, 真实级别为C9\n",
      "预测值为0.000001786, 真实值为0.000009294\n",
      "预测错误！预测级别为C2, 真实级别为M3\n",
      "预测值为0.000001915, 真实值为0.000030588\n",
      "预测部分正确！预测级别为C2, 真实级别为C7\n",
      "预测值为0.000001959, 真实值为0.000007412\n",
      "预测错误！预测级别为C2, 真实级别为M8\n",
      "预测值为0.000001932, 真实值为0.000077647\n",
      "预测错误！预测级别为C2, 真实级别为M8\n",
      "预测值为0.000001928, 真实值为0.000077647\n",
      "预测错误！预测级别为C2, 真实级别为M1\n",
      "预测值为0.000001895, 真实值为0.000011765\n",
      "预测错误！预测级别为C2, 真实级别为M6\n",
      "预测值为0.000001781, 真实值为0.000061176\n",
      "预测错误！预测级别为C2, 真实级别为M6\n",
      "预测值为0.000002352, 真实值为0.000061176\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001904, 真实值为0.000002471\n",
      "预测部分正确！预测级别为C2, 真实级别为C5\n",
      "预测值为0.000001918, 真实值为0.000005176\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000001925, 真实值为0.000003176\n",
      "预测部分正确！预测级别为C2, 真实级别为C9\n",
      "预测值为0.000001923, 真实值为0.000008824\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000002005, 真实值为0.000002941\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000002352, 真实值为0.000002941\n",
      "预测部分正确！预测级别为C2, 真实级别为C6\n",
      "预测值为0.000001918, 真实值为0.000006353\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000001915, 真实值为0.000002941\n",
      "预测错误！预测级别为C2, 真实级别为M1\n",
      "预测值为0.000001977, 真实值为0.000011765\n",
      "预测部分正确！预测级别为C2, 真实级别为C5\n",
      "预测值为0.000001962, 真实值为0.000004941\n",
      "预测部分正确！预测级别为C2, 真实级别为C5\n",
      "预测值为0.000002352, 真实值为0.000004941\n",
      "预测部分正确！预测级别为C2, 真实级别为C8\n",
      "预测值为0.000001929, 真实值为0.000008235\n",
      "预测部分正确！预测级别为C2, 真实级别为C8\n",
      "预测值为0.000002028, 真实值为0.000008235\n",
      "预测错误！预测级别为C2, 真实级别为M1\n",
      "预测值为0.000001995, 真实值为0.000009882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/aiia/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/anaconda3/envs/aiia/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/anaconda3/envs/aiia/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/anaconda3/envs/aiia/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/anaconda3/envs/aiia/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/anaconda3/envs/aiia/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/anaconda3/envs/aiia/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/anaconda3/envs/aiia/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试 Step [9/20], MSE: 0.000000000384\n",
      "预测错误！预测级别为C2, 真实级别为M3\n",
      "预测值为0.000001941, 真实值为0.000030588\n",
      "预测错误！预测级别为C2, 真实级别为M3\n",
      "预测值为0.000001903, 真实值为0.000030588\n",
      "预测部分正确！预测级别为C2, 真实级别为C9\n",
      "预测值为0.000001962, 真实值为0.000009176\n",
      "预测错误！预测级别为C2, 真实级别为M4\n",
      "预测值为0.000001910, 真实值为0.000044706\n",
      "预测错误！预测级别为C2, 真实级别为M4\n",
      "预测值为0.000002352, 真实值为0.000044706\n",
      "预测部分正确！预测级别为C2, 真实级别为C9\n",
      "预测值为0.000001935, 真实值为0.000008824\n",
      "预测错误！预测级别为C2, 真实级别为M1\n",
      "预测值为0.000002352, 真实值为0.000012941\n",
      "预测错误！预测级别为C2, 真实级别为M1\n",
      "预测值为0.000001926, 真实值为0.000012941\n",
      "预测部分正确！预测级别为C2, 真实级别为C8\n",
      "预测值为0.000001949, 真实值为0.000007647\n",
      "预测错误！预测级别为C2, 真实级别为M2\n",
      "预测值为0.000001925, 真实值为0.000022353\n",
      "预测错误！预测级别为C2, 真实级别为M2\n",
      "预测值为0.000002020, 真实值为0.000022353\n",
      "预测部分正确！预测级别为C2, 真实级别为C9\n",
      "预测值为0.000001958, 真实值为0.000009294\n",
      "预测部分正确！预测级别为C2, 真实级别为C9\n",
      "预测值为0.000001943, 真实值为0.000009294\n",
      "预测部分正确！预测级别为C2, 真实级别为C4\n",
      "预测值为0.000001975, 真实值为0.000004235\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000002010, 真实值为0.000002706\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000002019, 真实值为0.000001765\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001944, 真实值为0.000001765\n",
      "预测错误！预测级别为C2, 真实级别为M2\n",
      "预测值为0.000002352, 真实值为0.000020000\n",
      "预测错误！预测级别为C2, 真实级别为M3\n",
      "预测值为0.000001955, 真实值为0.000027059\n",
      "预测错误！预测级别为C2, 真实级别为M3\n",
      "预测值为0.000001966, 真实值为0.000027059\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000002039, 真实值为0.000001412\n",
      "预测错误！预测级别为C2, 真实级别为M2\n",
      "预测值为0.000002352, 真实值为0.000021176\n",
      "预测部分正确！预测级别为C2, 真实级别为C8\n",
      "预测值为0.000001886, 真实值为0.000007765\n",
      "预测错误！预测级别为C2, 真实级别为M1\n",
      "预测值为0.000001896, 真实值为0.000011765\n",
      "预测错误！预测级别为C2, 真实级别为M2\n",
      "预测值为0.000001953, 真实值为0.000024706\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001899, 真实值为0.000002471\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000002352, 真实值为0.000002588\n",
      "预测错误！预测级别为C2, 真实级别为M2\n",
      "预测值为0.000002003, 真实值为0.000021176\n",
      "预测错误！预测级别为C2, 真实级别为M1\n",
      "预测值为0.000001896, 真实值为0.000012941\n",
      "预测错误！预测级别为C2, 真实级别为M2\n",
      "预测值为0.000001912, 真实值为0.000018824\n",
      "预测错误！预测级别为C2, 真实级别为M3\n",
      "预测值为0.000001947, 真实值为0.000027059\n",
      "预测错误！预测级别为C2, 真实级别为M4\n",
      "预测值为0.000001928, 真实值为0.000043529\n",
      "测试 Step [10/20], MSE: 0.000000001000\n",
      "预测错误！预测级别为C2, 真实级别为M4\n",
      "预测值为0.000001901, 真实值为0.000043529\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001980, 真实值为0.000001882\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000002088, 真实值为0.000002824\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000002016, 真实值为0.000002235\n",
      "预测错误！预测级别为C2, 真实级别为M1\n",
      "预测值为0.000001960, 真实值为0.000012941\n",
      "预测错误！预测级别为C2, 真实级别为M1\n",
      "预测值为0.000002352, 真实值为0.000012941\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001988, 真实值为0.000002353\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001894, 真实值为0.000001765\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001981, 真实值为0.000001765\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000001916, 真实值为0.000003176\n",
      "预测部分正确！预测级别为C2, 真实级别为C5\n",
      "预测值为0.000001950, 真实值为0.000005059\n",
      "预测部分正确！预测级别为C2, 真实级别为C5\n",
      "预测值为0.000001905, 真实值为0.000005059\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000002055, 真实值为0.000003176\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000001933, 真实值为0.000001294\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000001929, 真实值为0.000002706\n",
      "预测错误！预测级别为C2, 真实级别为M3\n",
      "预测值为0.000001908, 真实值为0.000030588\n",
      "预测错误！预测级别为C2, 真实级别为M3\n",
      "预测值为0.000002352, 真实值为0.000030588\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000001934, 真实值为0.000001412\n",
      "预测部分正确！预测级别为C2, 真实级别为C9\n",
      "预测值为0.000001945, 真实值为0.000008941\n",
      "预测部分正确！预测级别为C2, 真实级别为C9\n",
      "预测值为0.000001934, 真实值为0.000008941\n",
      "预测错误！预测级别为C2, 真实级别为M2\n",
      "预测值为0.000001919, 真实值为0.000023529\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000002030, 真实值为0.000001529\n",
      "预测错误！预测级别为C2, 真实级别为M2\n",
      "预测值为0.000001942, 真实值为0.000024706\n",
      "预测错误！预测级别为C2, 真实级别为M2\n",
      "预测值为0.000002352, 真实值为0.000024706\n",
      "预测错误！预测级别为C2, 真实级别为X1\n",
      "预测值为0.000001911, 真实值为0.000117647\n",
      "预测错误！预测级别为C2, 真实级别为X1\n",
      "预测值为0.000001987, 真实值为0.000117647\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000001959, 真实值为0.000001412\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001914, 真实值为0.000002118\n",
      "预测部分正确！预测级别为C2, 真实级别为C8\n",
      "预测值为0.000002027, 真实值为0.000007529\n",
      "预测部分正确！预测级别为C2, 真实级别为C5\n",
      "预测值为0.000001894, 真实值为0.000004588\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000002039, 真实值为0.000002353\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000001988, 真实值为0.000002824\n",
      "测试 Step [11/20], MSE: 0.000000000018\n",
      "预测部分正确！预测级别为C2, 真实级别为C4\n",
      "预测值为0.000001985, 真实值为0.000004235\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001944, 真实值为0.000002235\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000001998, 真实值为0.000001176\n",
      "预测部分正确！预测级别为C2, 真实级别为C4\n",
      "预测值为0.000002016, 真实值为0.000004471\n",
      "预测部分正确！预测级别为C2, 真实级别为C6\n",
      "预测值为0.000002054, 真实值为0.000006000\n",
      "预测部分正确！预测级别为C2, 真实级别为C6\n",
      "预测值为0.000001932, 真实值为0.000006000\n",
      "预测部分正确！预测级别为C2, 真实级别为C5\n",
      "预测值为0.000001897, 真实值为0.000005059\n",
      "预测错误！预测级别为C2, 真实级别为B8\n",
      "预测值为0.000001937, 真实值为0.000000753\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000001900, 真实值为0.000001071\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000002352, 真实值为0.000001071\n",
      "预测错误！预测级别为C2, 真实级别为B7\n",
      "预测值为0.000001928, 真实值为0.000000682\n",
      "预测部分正确！预测级别为C2, 真实级别为C5\n",
      "预测值为0.000001968, 真实值为0.000005176\n",
      "预测部分正确！预测级别为C2, 真实级别为C5\n",
      "预测值为0.000001987, 真实值为0.000005176\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001958, 真实值为0.000001529\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000001953, 真实值为0.000001106\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001901, 真实值为0.000002118\n",
      "预测错误！预测级别为C2, 真实级别为B7\n",
      "预测值为0.000001937, 真实值为0.000000706\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000001895, 真实值为0.000001294\n",
      "预测部分正确！预测级别为C2, 真实级别为C9\n",
      "预测值为0.000001904, 真实值为0.000009176\n",
      "预测部分正确！预测级别为C2, 真实级别为C8\n",
      "预测值为0.000001919, 真实值为0.000007529\n",
      "预测部分正确！预测级别为C2, 真实级别为C6\n",
      "预测值为0.000001816, 真实值为0.000005882\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000001916, 真实值为0.000002941\n",
      "预测部分正确！预测级别为C2, 真实级别为C4\n",
      "预测值为0.000001903, 真实值为0.000004471\n",
      "预测部分正确！预测级别为C2, 真实级别为C6\n",
      "预测值为0.000001956, 真实值为0.000005765\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001908, 真实值为0.000002471\n",
      "预测错误！预测级别为C2, 真实级别为M2\n",
      "预测值为0.000001953, 真实值为0.000015294\n",
      "预测错误！预测级别为C2, 真实级别为M2\n",
      "预测值为0.000001825, 真实值为0.000015294\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001887, 真实值为0.000001765\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000002352, 真实值为0.000001765\n",
      "预测错误！预测级别为C2, 真实级别为B6\n",
      "预测值为0.000002006, 真实值为0.000000576\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001950, 真实值为0.000002471\n",
      "预测错误！预测级别为C2, 真实级别为B5\n",
      "预测值为0.000001978, 真实值为0.000000459\n",
      "测试 Step [12/20], MSE: 0.000000000011\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000002009, 真实值为0.000001529\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000001931, 真实值为0.000001412\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000002352, 真实值为0.000001412\n",
      "预测错误！预测级别为C2, 真实级别为M2\n",
      "预测值为0.000002020, 真实值为0.000015294\n",
      "预测部分正确！预测级别为C2, 真实级别为C6\n",
      "预测值为0.000001940, 真实值为0.000005882\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001904, 真实值为0.000002471\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001948, 真实值为0.000002471\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000001908, 真实值为0.000001176\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000002352, 真实值为0.000001176\n",
      "预测错误！预测级别为C2, 真实级别为B9\n",
      "预测值为0.000001889, 真实值为0.000000929\n",
      "预测部分正确！预测级别为C2, 真实级别为C5\n",
      "预测值为0.000001940, 真实值为0.000004706\n",
      "预测错误！预测级别为C2, 真实级别为B4\n",
      "预测值为0.000001912, 真实值为0.000000400\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000001881, 真实值为0.000001176\n",
      "预测错误！预测级别为C2, 真实级别为B6\n",
      "预测值为0.000001902, 真实值为0.000000624\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001946, 真实值为0.000002118\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001991, 真实值为0.000002118\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000001985, 真实值为0.000002588\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000001993, 真实值为0.000002588\n",
      "预测部分正确！预测级别为C2, 真实级别为C4\n",
      "预测值为0.000001935, 真实值为0.000004000\n",
      "预测部分正确！预测级别为C2, 真实级别为C5\n",
      "预测值为0.000001970, 真实值为0.000005059\n",
      "预测部分正确！预测级别为C2, 真实级别为C8\n",
      "预测值为0.000001950, 真实值为0.000007529\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001990, 真实值为0.000001765\n",
      "预测部分正确！预测级别为C2, 真实级别为C4\n",
      "预测值为0.000001917, 真实值为0.000003529\n",
      "预测部分正确！预测级别为C2, 真实级别为C4\n",
      "预测值为0.000001917, 真实值为0.000003882\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001967, 真实值为0.000002471\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000002352, 真实值为0.000002471\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000001909, 真实值为0.000002706\n",
      "预测部分正确！预测级别为C2, 真实级别为C8\n",
      "预测值为0.000001924, 真实值为0.000008353\n",
      "预测部分正确！预测级别为C2, 真实级别为C8\n",
      "预测值为0.000001949, 真实值为0.000008353\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001978, 真实值为0.000001765\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001927, 真实值为0.000002353\n",
      "预测部分正确！预测级别为C2, 真实级别为C4\n",
      "预测值为0.000001949, 真实值为0.000004118\n",
      "测试 Step [13/20], MSE: 0.000000015470\n",
      "预测部分正确！预测级别为C2, 真实级别为C6\n",
      "预测值为0.000001969, 真实值为0.000006471\n",
      "预测部分正确！预测级别为C2, 真实级别为C6\n",
      "预测值为0.000001964, 真实值为0.000006471\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000001941, 真实值为0.000001294\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000001989, 真实值为0.000001294\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000001992, 真实值为0.000003412\n",
      "预测部分正确！预测级别为C2, 真实级别为C4\n",
      "预测值为0.000001925, 真实值为0.000004235\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001941, 真实值为0.000002000\n",
      "预测错误！预测级别为C2, 真实级别为X2\n",
      "预测值为0.000001831, 真实值为0.000188235\n",
      "预测错误！预测级别为C2, 真实级别为M2\n",
      "预测值为0.000001908, 真实值为0.000016471\n",
      "预测错误！预测级别为C2, 真实级别为M1\n",
      "预测值为0.000001982, 真实值为0.000011412\n",
      "预测错误！预测级别为C2, 真实级别为X2\n",
      "预测值为0.000001738, 真实值为0.000235294\n",
      "预测错误！预测级别为C2, 真实级别为X2\n",
      "预测值为0.000001818, 真实值为0.000235294\n",
      "预测错误！预测级别为C2, 真实级别为M5\n",
      "预测值为0.000001863, 真实值为0.000045882\n",
      "预测错误！预测级别为C2, 真实级别为M4\n",
      "预测值为0.000001963, 真实值为0.000041176\n",
      "预测错误！预测级别为C2, 真实级别为M4\n",
      "预测值为0.000001962, 真实值为0.000041176\n",
      "预测错误！预测级别为C2, 真实级别为M1\n",
      "预测值为0.000001745, 真实值为0.000012941\n",
      "预测错误！预测级别为C2, 真实级别为X1\n",
      "预测值为0.000001888, 真实值为0.000102353\n",
      "预测错误！预测级别为C2, 真实级别为X1\n",
      "预测值为0.000001912, 真实值为0.000117647\n",
      "预测错误！预测级别为C2, 真实级别为X1\n",
      "预测值为0.000002352, 真实值为0.000117647\n",
      "预测错误！预测级别为C2, 真实级别为M1\n",
      "预测值为0.000001901, 真实值为0.000014118\n",
      "预测错误！预测级别为C2, 真实级别为M8\n",
      "预测值为0.000001751, 真实值为0.000078823\n",
      "预测错误！预测级别为C2, 真实级别为M1\n",
      "预测值为0.000002028, 真实值为0.000009647\n",
      "预测错误！预测级别为C2, 真实级别为M8\n",
      "预测值为0.000001899, 真实值为0.000077647\n",
      "预测错误！预测级别为C2, 真实级别为M8\n",
      "预测值为0.000002352, 真实值为0.000077647\n",
      "预测错误！预测级别为C2, 真实级别为M3\n",
      "预测值为0.000001948, 真实值为0.000027059\n",
      "预测错误！预测级别为C2, 真实级别为X4\n",
      "预测值为0.000001696, 真实值为0.000364706\n",
      "预测错误！预测级别为C2, 真实级别为X4\n",
      "预测值为0.000001725, 真实值为0.000364706\n",
      "预测错误！预测级别为C2, 真实级别为M8\n",
      "预测值为0.000001751, 真实值为0.000083529\n",
      "预测错误！预测级别为C2, 真实级别为X1\n",
      "预测值为0.000001849, 真实值为0.000129412\n",
      "预测错误！预测级别为C2, 真实级别为M5\n",
      "预测值为0.000001738, 真实值为0.000047059\n",
      "预测错误！预测级别为C2, 真实级别为M5\n",
      "预测值为0.000001706, 真实值为0.000047059\n",
      "预测错误！预测级别为C2, 真实级别为M2\n",
      "预测值为0.000001935, 真实值为0.000015294\n",
      "测试 Step [14/20], MSE: 0.000000003772\n",
      "预测错误！预测级别为C2, 真实级别为M2\n",
      "预测值为0.000001874, 真实值为0.000018824\n",
      "预测错误！预测级别为C2, 真实级别为X2\n",
      "预测值为0.000001699, 真实值为0.000235294\n",
      "预测错误！预测级别为C2, 真实级别为X2\n",
      "预测值为0.000001710, 真实值为0.000235294\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000001969, 真实值为0.000002941\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000001983, 真实值为0.000002824\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000001957, 真实值为0.000003412\n",
      "预测部分正确！预测级别为C2, 真实级别为C6\n",
      "预测值为0.000001950, 真实值为0.000006118\n",
      "预测部分正确！预测级别为C2, 真实级别为C6\n",
      "预测值为0.000002038, 真实值为0.000006118\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000001937, 真实值为0.000001294\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001970, 真实值为0.000002118\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000001987, 真实值为0.000003176\n",
      "预测错误！预测级别为C2, 真实级别为M7\n",
      "预测值为0.000002013, 真实值为0.000071765\n",
      "预测错误！预测级别为C2, 真实级别为M7\n",
      "预测值为0.000001927, 真实值为0.000071765\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000002014, 真实值为0.000002588\n",
      "预测部分正确！预测级别为C2, 真实级别为C5\n",
      "预测值为0.000002003, 真实值为0.000005176\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000002026, 真实值为0.000003294\n",
      "预测部分正确！预测级别为C2, 真实级别为C8\n",
      "预测值为0.000001958, 真实值为0.000007647\n",
      "预测部分正确！预测级别为C2, 真实级别为C9\n",
      "预测值为0.000001965, 真实值为0.000008824\n",
      "预测部分正确！预测级别为C2, 真实级别为C4\n",
      "预测值为0.000001981, 真实值为0.000004471\n",
      "预测部分正确！预测级别为C2, 真实级别为C5\n",
      "预测值为0.000001982, 真实值为0.000004706\n",
      "预测错误！预测级别为C2, 真实级别为M2\n",
      "预测值为0.000001940, 真实值为0.000021176\n",
      "预测错误！预测级别为C2, 真实级别为M2\n",
      "预测值为0.000001936, 真实值为0.000021176\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000001951, 真实值为0.000001129\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000002352, 真实值为0.000001129\n",
      "预测错误！预测级别为C2, 真实级别为M2\n",
      "预测值为0.000001982, 真实值为0.000015294\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000001968, 真实值为0.000002588\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000002021, 真实值为0.000001647\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000002352, 真实值为0.000001647\n",
      "预测部分正确！预测级别为C2, 真实级别为C5\n",
      "预测值为0.000001955, 真实值为0.000004706\n",
      "预测部分正确！预测级别为C2, 真实级别为C4\n",
      "预测值为0.000001986, 真实值为0.000004353\n",
      "预测错误！预测级别为C2, 真实级别为M2\n",
      "预测值为0.000001945, 真实值为0.000017647\n",
      "预测错误！预测级别为C2, 真实级别为M2\n",
      "预测值为0.000002352, 真实值为0.000017647\n",
      "测试 Step [15/20], MSE: 0.000000000405\n",
      "预测部分正确！预测级别为C2, 真实级别为C5\n",
      "预测值为0.000001946, 真实值为0.000004941\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000001967, 真实值为0.000003412\n",
      "预测部分正确！预测级别为C2, 真实级别为C4\n",
      "预测值为0.000001934, 真实值为0.000004471\n",
      "预测部分正确！预测级别为C2, 真实级别为C8\n",
      "预测值为0.000001942, 真实值为0.000007882\n",
      "预测部分正确！预测级别为C2, 真实级别为C8\n",
      "预测值为0.000001871, 真实值为0.000007882\n",
      "预测错误！预测级别为C2, 真实级别为M1\n",
      "预测值为0.000001963, 真实值为0.000009529\n",
      "预测错误！预测级别为C2, 真实级别为M1\n",
      "预测值为0.000001940, 真实值为0.000009529\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001946, 真实值为0.000001529\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000001916, 真实值为0.000001176\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001881, 真实值为0.000002118\n",
      "预测错误！预测级别为C2, 真实级别为M1\n",
      "预测值为0.000001934, 真实值为0.000010118\n",
      "预测错误！预测级别为C2, 真实级别为M1\n",
      "预测值为0.000002230, 真实值为0.000010118\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001935, 真实值为0.000002353\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001961, 真实值为0.000001529\n",
      "预测错误！预测级别为C2, 真实级别为M7\n",
      "预测值为0.000001974, 真实值为0.000065882\n",
      "预测错误！预测级别为C2, 真实级别为M7\n",
      "预测值为0.000001990, 真实值为0.000065882\n",
      "预测部分正确！预测级别为C2, 真实级别为C4\n",
      "预测值为0.000001910, 真实值为0.000003529\n",
      "预测部分正确！预测级别为C2, 真实级别为C4\n",
      "预测值为0.000001976, 真实值为0.000004000\n",
      "预测部分正确！预测级别为C2, 真实级别为C4\n",
      "预测值为0.000002042, 真实值为0.000004000\n",
      "预测错误！预测级别为C2, 真实级别为M1\n",
      "预测值为0.000002042, 真实值为0.000011294\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000001937, 真实值为0.000002824\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000001919, 真实值为0.000003059\n",
      "预测部分正确！预测级别为C2, 真实级别为C4\n",
      "预测值为0.000001978, 真实值为0.000003882\n",
      "预测错误！预测级别为C2, 真实级别为M6\n",
      "预测值为0.000001973, 真实值为0.000057647\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001947, 真实值为0.000002471\n",
      "预测错误！预测级别为C2, 真实级别为M3\n",
      "预测值为0.000001980, 真实值为0.000025882\n",
      "预测错误！预测级别为C2, 真实级别为M3\n",
      "预测值为0.000001975, 真实值为0.000025882\n",
      "预测部分正确！预测级别为C2, 真实级别为C4\n",
      "预测值为0.000001951, 真实值为0.000003882\n",
      "预测部分正确！预测级别为C2, 真实级别为C4\n",
      "预测值为0.000001946, 真实值为0.000003647\n",
      "预测部分正确！预测级别为C2, 真实级别为C4\n",
      "预测值为0.000002021, 真实值为0.000003647\n",
      "预测错误！预测级别为C2, 真实级别为M1\n",
      "预测值为0.000001959, 真实值为0.000011412\n",
      "预测部分正确！预测级别为C2, 真实级别为C7\n",
      "预测值为0.000001925, 真实值为0.000006588\n",
      "测试 Step [16/20], MSE: 0.000000000066\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001925, 真实值为0.000002000\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001957, 真实值为0.000001882\n",
      "预测部分正确！预测级别为C2, 真实级别为C7\n",
      "预测值为0.000001938, 真实值为0.000007176\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001952, 真实值为0.000002118\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000002054, 真实值为0.000002118\n",
      "预测部分正确！预测级别为C2, 真实级别为C4\n",
      "预测值为0.000002028, 真实值为0.000004353\n",
      "预测错误！预测级别为C2, 真实级别为B8\n",
      "预测值为0.000002015, 真实值为0.000000824\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000001937, 真实值为0.000003412\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000001852, 真实值为0.000003059\n",
      "预测错误！预测级别为C2, 真实级别为M1\n",
      "预测值为0.000001939, 真实值为0.000009765\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001842, 真实值为0.000001647\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001851, 真实值为0.000002235\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001965, 真实值为0.000001765\n",
      "预测部分正确！预测级别为C2, 真实级别为C7\n",
      "预测值为0.000002015, 真实值为0.000007059\n",
      "预测部分正确！预测级别为C2, 真实级别为C4\n",
      "预测值为0.000001965, 真实值为0.000003529\n",
      "预测部分正确！预测级别为C2, 真实级别为C4\n",
      "预测值为0.000001885, 真实值为0.000003529\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001824, 真实值为0.000001647\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001771, 真实值为0.000001647\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000002352, 真实值为0.000000976\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001973, 真实值为0.000002353\n",
      "预测错误！预测级别为C2, 真实级别为M3\n",
      "预测值为0.000001966, 真实值为0.000029412\n",
      "预测错误！预测级别为C2, 真实级别为M3\n",
      "预测值为0.000002059, 真实值为0.000029412\n",
      "预测错误！预测级别为C2, 真实级别为M2\n",
      "预测值为0.000001988, 真实值为0.000022353\n",
      "预测部分正确！预测级别为C2, 真实级别为C5\n",
      "预测值为0.000001974, 真实值为0.000004588\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000002005, 真实值为0.000001059\n",
      "预测部分正确！预测级别为C2, 真实级别为C7\n",
      "预测值为0.000001978, 真实值为0.000006941\n",
      "预测部分正确！预测级别为C2, 真实级别为C5\n",
      "预测值为0.000002007, 真实值为0.000005059\n",
      "预测部分正确！预测级别为C2, 真实级别为C5\n",
      "预测值为0.000001989, 真实值为0.000005059\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000002016, 真实值为0.000001141\n",
      "预测部分正确！预测级别为C2, 真实级别为C5\n",
      "预测值为0.000001989, 真实值为0.000004824\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000001953, 真实值为0.000002706\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000002352, 真实值为0.000002706\n",
      "测试 Step [17/20], MSE: 0.000000000268\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001983, 真实值为0.000001882\n",
      "预测错误！预测级别为C2, 真实级别为M3\n",
      "预测值为0.000001973, 真实值为0.000027059\n",
      "预测错误！预测级别为C2, 真实级别为M3\n",
      "预测值为0.000001950, 真实值为0.000027059\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001996, 真实值为0.000001765\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000001967, 真实值为0.000003176\n",
      "预测部分正确！预测级别为C2, 真实级别为C9\n",
      "预测值为0.000002352, 真实值为0.000009412\n",
      "预测错误！预测级别为C2, 真实级别为M2\n",
      "预测值为0.000002010, 真实值为0.000021176\n",
      "预测错误！预测级别为C2, 真实级别为M2\n",
      "预测值为0.000002352, 真实值为0.000021176\n",
      "预测错误！预测级别为C2, 真实级别为M1\n",
      "预测值为0.000002019, 真实值为0.000012941\n",
      "预测错误！预测级别为C2, 真实级别为M1\n",
      "预测值为0.000001997, 真实值为0.000012941\n",
      "预测错误！预测级别为C2, 真实级别为M6\n",
      "预测值为0.000001978, 真实值为0.000055294\n",
      "预测错误！预测级别为C2, 真实级别为M6\n",
      "预测值为0.000002352, 真实值为0.000055294\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000001964, 真实值为0.000002824\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000002352, 真实值为0.000002824\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000002003, 真实值为0.000001529\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001966, 真实值为0.000002353\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001970, 真实值为0.000002353\n",
      "预测部分正确！预测级别为C2, 真实级别为C8\n",
      "预测值为0.000002037, 真实值为0.000008000\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000002012, 真实值为0.000002941\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000002038, 真实值为0.000001294\n",
      "预测部分正确！预测级别为C2, 真实级别为C9\n",
      "预测值为0.000002064, 真实值为0.000008824\n",
      "预测部分正确！预测级别为C2, 真实级别为C9\n",
      "预测值为0.000002025, 真实值为0.000009176\n",
      "预测部分正确！预测级别为C2, 真实级别为C4\n",
      "预测值为0.000002013, 真实值为0.000003529\n",
      "预测错误！预测级别为C2, 真实级别为B8\n",
      "预测值为0.000002000, 真实值为0.000000824\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000002025, 真实值为0.000001294\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000002040, 真实值为0.000001294\n",
      "预测错误！预测级别为C2, 真实级别为B6\n",
      "预测值为0.000002023, 真实值为0.000000553\n",
      "预测错误！预测级别为C2, 真实级别为B7\n",
      "预测值为0.000002023, 真实值为0.000000706\n",
      "预测错误！预测级别为C2, 真实级别为B7\n",
      "预测值为0.000002009, 真实值为0.000000706\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001968, 真实值为0.000001882\n",
      "预测部分正确！预测级别为C2, 真实级别为C8\n",
      "预测值为0.000001956, 真实值为0.000007765\n",
      "预测错误！预测级别为C2, 真实级别为M2\n",
      "预测值为0.000001972, 真实值为0.000023529\n",
      "测试 Step [18/20], MSE: 0.000000000758\n",
      "预测错误！预测级别为C2, 真实级别为M2\n",
      "预测值为0.000001936, 真实值为0.000023529\n",
      "预测错误！预测级别为C2, 真实级别为M2\n",
      "预测值为0.000001984, 真实值为0.000022353\n",
      "预测错误！预测级别为C2, 真实级别为M2\n",
      "预测值为0.000002013, 真实值为0.000022353\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001971, 真实值为0.000001529\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000002352, 真实值为0.000001529\n",
      "预测错误！预测级别为C2, 真实级别为M9\n",
      "预测值为0.000002100, 真实值为0.000089412\n",
      "预测错误！预测级别为C2, 真实级别为M9\n",
      "预测值为0.000002006, 真实值为0.000089412\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000001977, 真实值为0.000001529\n",
      "预测错误！预测级别为C2, 真实级别为M1\n",
      "预测值为0.000002050, 真实值为0.000011765\n",
      "预测错误！预测级别为C2, 真实级别为M6\n",
      "预测值为0.000001973, 真实值为0.000064706\n",
      "预测错误！预测级别为C2, 真实级别为M1\n",
      "预测值为0.000001970, 真实值为0.000014118\n",
      "预测错误！预测级别为C2, 真实级别为M1\n",
      "预测值为0.000001939, 真实值为0.000014118\n",
      "预测部分正确！预测级别为C2, 真实级别为C5\n",
      "预测值为0.000001917, 真实值为0.000005412\n",
      "预测部分正确！预测级别为C2, 真实级别为C5\n",
      "预测值为0.000001954, 真实值为0.000004941\n",
      "预测部分正确！预测级别为C2, 真实级别为C5\n",
      "预测值为0.000002352, 真实值为0.000004941\n",
      "预测错误！预测级别为C2, 真实级别为M1\n",
      "预测值为0.000002043, 真实值为0.000010706\n",
      "预测错误！预测级别为C2, 真实级别为M6\n",
      "预测值为0.000002054, 真实值为0.000058824\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000002016, 真实值为0.000002118\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000001984, 真实值为0.000002824\n",
      "预测错误！预测级别为C2, 真实级别为B4\n",
      "预测值为0.000001965, 真实值为0.000000412\n",
      "预测错误！预测级别为C2, 真实级别为B4\n",
      "预测值为0.000001953, 真实值为0.000000412\n",
      "预测错误！预测级别为C2, 真实级别为B3\n",
      "预测值为0.000001965, 真实值为0.000000306\n",
      "预测错误！预测级别为C2, 真实级别为B4\n",
      "预测值为0.000001981, 真实值为0.000000353\n",
      "预测错误！预测级别为C2, 真实级别为B4\n",
      "预测值为0.000001927, 真实值为0.000000412\n",
      "预测错误！预测级别为C2, 真实级别为B4\n",
      "预测值为0.000001877, 真实值为0.000000412\n",
      "预测错误！预测级别为C2, 真实级别为B2\n",
      "预测值为0.000002035, 真实值为0.000000153\n",
      "预测错误！预测级别为C2, 真实级别为B1\n",
      "预测值为0.000001988, 真实值为0.000000141\n",
      "预测错误！预测级别为C2, 真实级别为B7\n",
      "预测值为0.000002002, 真实值为0.000000718\n",
      "预测错误！预测级别为C2, 真实级别为B7\n",
      "预测值为0.000001942, 真实值为0.000000718\n",
      "预测错误！预测级别为C2, 真实级别为B5\n",
      "预测值为0.000001942, 真实值为0.000000494\n",
      "预测错误！预测级别为C2, 真实级别为B2\n",
      "预测值为0.000001986, 真实值为0.000000176\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000002011, 真实值为0.000001094\n",
      "测试 Step [19/20], MSE: 0.000000000006\n",
      "预测错误！预测级别为C2, 真实级别为B2\n",
      "预测值为0.000001945, 真实值为0.000000176\n",
      "预测错误！预测级别为C2, 真实级别为B2\n",
      "预测值为0.000001961, 真实值为0.000000200\n",
      "预测错误！预测级别为C2, 真实级别为B3\n",
      "预测值为0.000002013, 真实值为0.000000329\n",
      "预测错误！预测级别为C2, 真实级别为B3\n",
      "预测值为0.000001952, 真实值为0.000000329\n",
      "预测错误！预测级别为C2, 真实级别为B3\n",
      "预测值为0.000002016, 真实值为0.000000306\n",
      "预测错误！预测级别为C2, 真实级别为B2\n",
      "预测值为0.000001997, 真实值为0.000000200\n",
      "预测错误！预测级别为C2, 真实级别为B2\n",
      "预测值为0.000002036, 真实值为0.000000200\n",
      "预测错误！预测级别为C2, 真实级别为B1\n",
      "预测值为0.000002352, 真实值为0.000000129\n",
      "预测错误！预测级别为C2, 真实级别为B2\n",
      "预测值为0.000001961, 真实值为0.000000153\n",
      "预测错误！预测级别为C2, 真实级别为B2\n",
      "预测值为0.000002352, 真实值为0.000000153\n",
      "预测错误！预测级别为C2, 真实级别为B2\n",
      "预测值为0.000001950, 真实值为0.000000247\n",
      "预测错误！预测级别为C2, 真实级别为B2\n",
      "预测值为0.000001967, 真实值为0.000000247\n",
      "预测错误！预测级别为C2, 真实级别为B2\n",
      "预测值为0.000002352, 真实值为0.000000200\n",
      "预测错误！预测级别为C2, 真实级别为B2\n",
      "预测值为0.000002005, 真实值为0.000000153\n",
      "预测错误！预测级别为C2, 真实级别为B6\n",
      "预测值为0.000001962, 真实值为0.000000565\n",
      "预测错误！预测级别为C2, 真实级别为B2\n",
      "预测值为0.000002006, 真实值为0.000000247\n",
      "预测错误！预测级别为C2, 真实级别为B2\n",
      "预测值为0.000001999, 真实值为0.000000153\n",
      "预测错误！预测级别为C2, 真实级别为B1\n",
      "预测值为0.000001982, 真实值为0.000000141\n",
      "预测错误！预测级别为C2, 真实级别为B2\n",
      "预测值为0.000002001, 真实值为0.000000165\n",
      "预测错误！预测级别为C2, 真实级别为B2\n",
      "预测值为0.000002005, 真实值为0.000000153\n",
      "预测错误！预测级别为C2, 真实级别为B2\n",
      "预测值为0.000001980, 真实值为0.000000176\n",
      "预测错误！预测级别为C2, 真实级别为B2\n",
      "预测值为0.000002008, 真实值为0.000000176\n",
      "预测错误！预测级别为C2, 真实级别为B2\n",
      "预测值为0.000001969, 真实值为0.000000153\n",
      "预测错误！预测级别为C2, 真实级别为B2\n",
      "预测值为0.000001985, 真实值为0.000000153\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000001999, 真实值为0.000003176\n",
      "预测部分正确！预测级别为C2, 真实级别为C4\n",
      "预测值为0.000001999, 真实值为0.000003529\n",
      "预测错误！预测级别为C2, 真实级别为B8\n",
      "预测值为0.000001989, 真实值为0.000000753\n",
      "预测部分正确！预测级别为C2, 真实级别为C9\n",
      "预测值为0.000001991, 真实值为0.000009412\n",
      "预测部分正确！预测级别为C2, 真实级别为C9\n",
      "预测值为0.000001982, 真实值为0.000009412\n",
      "预测错误！预测级别为C2, 真实级别为B2\n",
      "预测值为0.000001981, 真实值为0.000000212\n",
      "预测错误！预测级别为C2, 真实级别为B3\n",
      "预测值为0.000001971, 真实值为0.000000341\n",
      "预测错误！预测级别为C2, 真实级别为B5\n",
      "预测值为0.000001996, 真实值为0.000000506\n",
      "测试 Step [20/20], MSE: 0.000000115539\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000001995, 真实值为0.000002941\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000001991, 真实值为0.000002941\n",
      "预测错误！预测级别为C2, 真实级别为B5\n",
      "预测值为0.000001932, 真实值为0.000000482\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000001985, 真实值为0.000001106\n",
      "预测错误！预测级别为C2, 真实级别为B5\n",
      "预测值为0.000001981, 真实值为0.000000518\n",
      "预测部分正确！预测级别为C2, 真实级别为C3\n",
      "预测值为0.000002010, 真实值为0.000002941\n",
      "预测错误！预测级别为C2, 真实级别为M1\n",
      "预测值为0.000002063, 真实值为0.000012941\n",
      "预测错误！预测级别为C2, 真实级别为X1\n",
      "预测值为0.000002010, 真实值为0.000095294\n",
      "预测错误！预测级别为C2, 真实级别为X1\n",
      "预测值为0.000002352, 真实值为0.000095294\n",
      "预测错误！预测级别为C2, 真实级别为M2\n",
      "预测值为0.000001995, 真实值为0.000017647\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000002007, 真实值为0.000001412\n",
      "预测错误！预测级别为C2, 真实级别为M3\n",
      "预测值为0.000002352, 真实值为0.000027059\n",
      "预测错误！预测级别为C2, 真实级别为X3\n",
      "预测值为0.000002352, 真实值为0.000258824\n",
      "预测错误！预测级别为C2, 真实级别为M6\n",
      "预测值为0.000001996, 真实值为0.000064706\n",
      "预测错误！预测级别为C2, 真实级别为M6\n",
      "预测值为0.000001850, 真实值为0.000064706\n",
      "预测错误！预测级别为C2, 真实级别为M5\n",
      "预测值为0.000001968, 真实值为0.000049412\n",
      "预测错误！预测级别为C2, 真实级别为M2\n",
      "预测值为0.000002005, 真实值为0.000020000\n",
      "预测错误！预测级别为C2, 真实级别为M1\n",
      "预测值为0.000002010, 真实值为0.000014118\n",
      "预测部分正确！预测级别为C2, 真实级别为C1\n",
      "预测值为0.000001945, 真实值为0.000001294\n",
      "预测错误！预测级别为C2, 真实级别为M9\n",
      "预测值为0.000001967, 真实值为0.000085882\n",
      "预测错误！预测级别为C2, 真实级别为M4\n",
      "预测值为0.000002017, 真实值为0.000043529\n",
      "预测错误！预测级别为C2, 真实级别为M4\n",
      "预测值为0.000001959, 真实值为0.000043529\n",
      "预测错误！预测级别为C2, 真实级别为M3\n",
      "预测值为0.000002352, 真实值为0.000034118\n",
      "预测错误！预测级别为C2, 真实级别为X11\n",
      "预测值为0.000001887, 真实值为0.001094118\n",
      "预测错误！预测级别为C2, 真实级别为X11\n",
      "预测值为0.000001935, 真实值为0.001094118\n",
      "预测正确！预测级别为C2, 真实级别为C2\n",
      "预测值为0.000002014, 真实值为0.000002118\n",
      "预测错误！预测级别为C2, 真实级别为X2\n",
      "预测值为0.000001983, 真实值为0.000152941\n",
      "预测错误！预测级别为C2, 真实级别为X2\n",
      "预测值为0.000001976, 真实值为0.000152941\n",
      "预测错误！预测级别为C2, 真实级别为X10\n",
      "预测值为0.000002038, 真实值为0.000964706\n",
      "预测错误！预测级别为C2, 真实级别为M4\n",
      "预测值为0.000001915, 真实值为0.000044706\n",
      "测试集准确率: 0.2638\n",
      "MSE:3.650779567293972e-09\n",
      "TSS: nan, HSS: 0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/aiia/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/anaconda3/envs/aiia/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/anaconda3/envs/aiia/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/anaconda3/envs/aiia/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([30])) that is different to the input size (torch.Size([30, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/tmp/ipykernel_882038/4135423889.py:35: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return tp / (tp + fn) - fp / (fp + tn)\n"
     ]
    }
   ],
   "source": [
    "#测试\n",
    "\n",
    "#加载模型\n",
    "model_state_dict = torch.load('CNN.pth')\n",
    "model.to(device)\n",
    "model.eval()  # 设置模型为评估模式\n",
    "criterion = nn.MSELoss()\n",
    "test_loss = 0.0\n",
    "right = 0\n",
    "num = 0\n",
    "acc_mse = 0\n",
    "for i, (inputs, labels) in enumerate(test_loader):\n",
    "    inputs, labels = inputs.cpu(), labels.cpu()\n",
    "    labels = np.exp(denormalize_peak_flux(labels, scaler))\n",
    "    print(np.mean(labels))\n",
    "\n",
    "with torch.no_grad():  # 禁用梯度计算，节省内存\n",
    "    for i, (inputs, labels) in enumerate(test_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        predicted = outputs.data.cpu().numpy()\n",
    "        labels = labels.cpu().numpy()\n",
    "        predicted = np.exp(denormalize_peak_flux(predicted, scaler))\n",
    "        labels = np.exp(denormalize_peak_flux(labels, scaler))\n",
    "        '''\n",
    "        for prediction in predicted:\n",
    "            prediction[0] = flux_denormalizer(prediction[0])\n",
    "        labels = [flux_denormalizer(label) for label in labels]\n",
    "        '''\n",
    "        mse = np.mean((predicted - labels) ** 2)\n",
    "        acc_mse += mse\n",
    "        print(f'测试 Step [{i+1}/{len(test_loader)}], MSE: {mse:.12f}')\n",
    "\n",
    "        predicted_class = [flux_to_class(p) for p in predicted]\n",
    "        labels_class = [flux_to_class(l) for l in labels]\n",
    "\n",
    "        #分析耀斑的级别\n",
    "        for j in range(len(predicted)):\n",
    "            num += 1\n",
    "            '''\n",
    "            if abs(predicted[j] - labels[j]) < 1e-6:\n",
    "                right += 1\n",
    "                print(f'预测正确！预测级别为{predicted_class[j]}, 真实级别为{labels_class[j]}')\n",
    "                print(f'预测值为{predicted[j]:.9f}, 真实值为{labels[j]:.9f}')\n",
    "            '''\n",
    "            if predicted_class[j] == labels_class[j]:\n",
    "                right += 1\n",
    "                print(f'预测正确！预测级别为{predicted_class[j]}, 真实级别为{labels_class[j]}')\n",
    "                print(f'预测值为{predicted[j]:.9f}, 真实值为{labels[j]:.9f}')\n",
    "            elif predicted_class[j][0] == labels_class[j][0]:\n",
    "                right += 0.3\n",
    "                print(f'预测部分正确！预测级别为{predicted_class[j]}, 真实级别为{labels_class[j]}')\n",
    "                print(f'预测值为{predicted[j]:.9f}, 真实值为{labels[j]:.9f}')\n",
    "            else:\n",
    "                right += 0\n",
    "                print(f'预测错误！预测级别为{predicted_class[j]}, 真实级别为{labels_class[j]}')\n",
    "                print(f'预测值为{predicted[j]:.9f}, 真实值为{labels[j]:.9f}')\n",
    "            \n",
    "accuracy = right / num\n",
    "print(f'测试集准确率: {accuracy:.4f}')\n",
    "print(f'MSE:{acc_mse/40}')\n",
    "#print(f'平均测试损失: {test_loss / len(test_loader):.12f}')\n",
    "\n",
    "#计算TSS和HSS\n",
    "tss =  true_skill_statistic(np.array(predicted), np.array(labels), threshold='M')\n",
    "hss =  heidke_skill_score(np.array(predicted), np.array(labels), threshold='M')\n",
    "print(f'TSS: {tss:.4f}, HSS: {hss:.4f}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
